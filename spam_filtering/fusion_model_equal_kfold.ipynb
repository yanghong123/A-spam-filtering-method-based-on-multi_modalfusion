{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aqts/yangHong/first-spam-experiment/data/hybrid_email_dataset_equal/text/\n",
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "Processing text dataset\n",
      "1 : Subject: palm pilot request\n",
      "je\n",
      "ham\n",
      "2 : Subject: invoice processing\n",
      "as\n",
      "ham\n",
      "3 : Subject: north baja project\n",
      "ac\n",
      "ham\n",
      "4 : Subject: long term outlook\n",
      "fyi\n",
      "ham\n",
      "5 : Subject: account assignment li\n",
      "ham\n",
      "6 : Subject: pg & e deliveries\n",
      "her\n",
      "ham\n",
      "7 : Subject: travel announcement\n",
      "e\n",
      "ham\n",
      "8 : Subject: pac enrollment\n",
      "last y\n",
      "ham\n",
      "9 : Subject: ferc gives guardian f\n",
      "ham\n",
      "10 : Subject: [ fwd : re : graco co\n",
      "ham\n",
      "11 : Subject: nymex info on desktop\n",
      "ham\n",
      "12 : Subject: competing pipelines\n",
      "o\n",
      "ham\n",
      "13 : Subject: organizational announ\n",
      "ham\n",
      "14 : Subject: a few items from jean\n",
      "ham\n",
      "15 : Subject: cera says average nat\n",
      "ham\n",
      "16 : Subject: tw cash for fuel issu\n",
      "ham\n",
      "17 : Subject: cera says average nat\n",
      "ham\n",
      "18 : Subject: contact list\n",
      "please f\n",
      "ham\n",
      "19 : Subject: capacity options on t\n",
      "ham\n",
      "20 : Subject: cost of service annou\n",
      "ham\n",
      "21 : Subject: a simple communicatio\n",
      "ham\n",
      "22 : Subject: restore address book \n",
      "ham\n",
      "23 : Subject: wait list notificatio\n",
      "ham\n",
      "24 : Subject: all employee surplus \n",
      "ham\n",
      "25 : Subject: reimbursement of indi\n",
      "ham\n",
      "26 : Subject: palm pilots - gone ! \n",
      "ham\n",
      "27 : Subject: enron in action 06 . \n",
      "ham\n",
      "28 : Subject: et & s business devel\n",
      "ham\n",
      "29 : Subject: two announcements on \n",
      "ham\n",
      "30 : Subject: tw map program\n",
      "- - - \n",
      "ham\n",
      "31 : Subject: update - reimbursemen\n",
      "ham\n",
      "32 : Subject: happy hour for patti \n",
      "ham\n",
      "33 : Subject: sap coding quick refe\n",
      "ham\n",
      "34 : Subject: caithness big sandy ,\n",
      "ham\n",
      "35 : Subject: etv is here\n",
      "well , wh\n",
      "ham\n",
      "36 : Subject: the dot is coming\n",
      "tha\n",
      "ham\n",
      "37 : Subject: dp question\n",
      "jeff :\n",
      "yo\n",
      "ham\n",
      "38 : Subject: so , what are we sayi\n",
      "ham\n",
      "39 : Subject: updated contact list\n",
      "\n",
      "ham\n",
      "40 : Subject: enron building notice\n",
      "ham\n",
      "41 : Subject: enronoptions - your s\n",
      "ham\n",
      "42 : Subject: sap expense reports -\n",
      "ham\n",
      "43 : Subject: class confirmation - \n",
      "ham\n",
      "44 : Subject: derivatives class sch\n",
      "ham\n",
      "45 : Subject: the dot is here .\n",
      "it \n",
      "ham\n",
      "46 : Subject: pipeline updates brow\n",
      "ham\n",
      "47 : Subject: kmi ( ngpl ) order 63\n",
      "ham\n",
      "48 : Subject: tw rofr postings\n",
      "lega\n",
      "ham\n",
      "49 : Subject: contacts\n",
      "here are my \n",
      "ham\n",
      "50 : Subject: calpine power plant o\n",
      "ham\n",
      "51 : Subject: development center cl\n",
      "ham\n",
      "52 : Subject: enron in action\n",
      "ivolu\n",
      "ham\n",
      "53 : Subject: re : dental coverage\n",
      "\n",
      "ham\n",
      "54 : Subject: contract options\n",
      "as w\n",
      "ham\n",
      "55 : Subject: \" 100 best companies \n",
      "ham\n",
      "56 : Subject: throughput forecast\n",
      "i\n",
      "ham\n",
      "57 : Subject: nesa / hea merger\n",
      "- m\n",
      "ham\n",
      "58 : Subject: hea / nesa merger\n",
      "mic\n",
      "ham\n",
      "59 : Subject: dashboard enhancement\n",
      "ham\n",
      "60 : Subject: final final meeting\n",
      "d\n",
      "ham\n",
      "61 : Subject: eol meeting on reg is\n",
      "ham\n",
      "62 : Subject: commitment to environ\n",
      "ham\n",
      "63 : Subject: financial wellness wo\n",
      "ham\n",
      "64 : Subject: today : 2000 united w\n",
      "ham\n",
      "65 : Subject: re : ios requests\n",
      "com\n",
      "ham\n",
      "66 : Subject: operator imbalances\n",
      "r\n",
      "ham\n",
      "67 : Subject: re : final final meet\n",
      "ham\n",
      "68 : Subject: united way - \" the ga\n",
      "ham\n",
      "69 : Subject: tw bullets 8 / 11\n",
      "int\n",
      "ham\n",
      "70 : Subject: i didn ' t think i wa\n",
      "ham\n",
      "71 : Subject: re : ios duke energy\n",
      "\n",
      "ham\n",
      "72 : Subject: transport options pro\n",
      "ham\n",
      "73 : Subject: game show support\n",
      "hey\n",
      "ham\n",
      "74 : Subject: certificate status re\n",
      "ham\n",
      "75 : Subject: new tw price points\n",
      "w\n",
      "ham\n",
      "76 : Subject: ethink about it : aug\n",
      "ham\n",
      "77 : Subject: 2000 chairman ' s awa\n",
      "ham\n",
      "78 : Subject: conoco\n",
      "dale - marketi\n",
      "ham\n",
      "79 : Subject: code of ethics\n",
      "as enr\n",
      "ham\n",
      "80 : Subject: revised hurricane for\n",
      "ham\n",
      "81 : Subject: the new peoplefinder \n",
      "ham\n",
      "82 : Subject: organizational study\n",
      "\n",
      "ham\n",
      "83 : Subject: moneycentral : 6 rout\n",
      "ham\n",
      "84 : Subject: nesa annual meeting b\n",
      "ham\n",
      "85 : Subject: dell computer financi\n",
      "ham\n",
      "86 : Subject: trip report\n",
      "summary o\n",
      "ham\n",
      "87 : Subject: customer meeting\n",
      "ngts\n",
      "ham\n",
      "88 : Subject: privileged & confiden\n",
      "ham\n",
      "89 : Subject: your investment at wo\n",
      "ham\n",
      "90 : Subject: enron in action 8 . 2\n",
      "ham\n",
      "91 : Subject: fw : e - mail charges\n",
      "ham\n",
      "92 : Subject: resubscription 2001 p\n",
      "ham\n",
      "93 : Subject: 2001 plan locations\n",
      "h\n",
      "ham\n",
      "94 : Subject: el paso articles toda\n",
      "ham\n",
      "95 : Subject: development center co\n",
      "ham\n",
      "96 : Subject: alliance pipeline con\n",
      "ham\n",
      "97 : Subject: monthly briefing : a \n",
      "ham\n",
      "98 : Subject: basis blowout\n",
      "fyi - h\n",
      "ham\n",
      "99 : Subject: needles / topock pric\n",
      "ham\n",
      "100 : Subject: memo to gpg managemen\n",
      "ham\n",
      "101 : Subject: organizational announ\n",
      "ham\n",
      "102 : Subject: el paso south mainlin\n",
      "ham\n",
      "103 : Subject: revised transportatio\n",
      "ham\n",
      "104 : Subject: transport options pro\n",
      "ham\n",
      "105 : Subject: re : bid week back - \n",
      "ham\n",
      "106 : Subject: fw : revised\n",
      "michelle\n",
      "ham\n",
      "107 : Subject: bullets 8 / 25\n",
      "el pas\n",
      "ham\n",
      "108 : Subject: revised bullets 8 / 2\n",
      "ham\n",
      "109 : Subject: nesa annual mtg . bro\n",
      "ham\n",
      "110 : Subject: board announcement\n",
      "th\n",
      "ham\n",
      "111 : Subject: breaking news\n",
      "check o\n",
      "ham\n",
      "112 : Subject: ethink about it : aug\n",
      "ham\n",
      "113 : Subject: august , 2000 park an\n",
      "ham\n",
      "114 : Subject: natural gas intellige\n",
      "ham\n",
      "115 : Subject: re : oneok contract\n",
      "t\n",
      "ham\n",
      "116 : Subject: pancanadian buys mont\n",
      "ham\n",
      "117 : Subject: storage report gri - \n",
      "ham\n",
      "118 : Subject: we hear you .\n",
      "you ask\n",
      "ham\n",
      "119 : Subject: gpg ' s day of caring\n",
      "ham\n",
      "120 : Subject: order 637 internet tr\n",
      "ham\n",
      "121 : Subject: 27258\n",
      "i would like to\n",
      "ham\n",
      "122 : Subject: nasa weather news\n",
      "la \n",
      "ham\n",
      "123 : Subject: development center co\n",
      "ham\n",
      "124 : Subject: order 637 internet tr\n",
      "ham\n",
      "125 : Subject: museum of fine arts 2\n",
      "ham\n",
      "126 : Subject: transwestern weekly r\n",
      "ham\n",
      "127 : Subject: tw weekly , 8 - 31 - \n",
      "ham\n",
      "128 : Subject: outstanding contracts\n",
      "ham\n",
      "129 : Subject: bullets 9 / 1\n",
      "el paso\n",
      "ham\n",
      "130 : Subject: august 2000 - park an\n",
      "ham\n",
      "131 : Subject: wisconsin winter fuel\n",
      "ham\n",
      "132 : Subject: linda jenkins on \" je\n",
      "ham\n",
      "133 : Subject: ethink about it : sep\n",
      "ham\n",
      "134 : Subject: hea ' s 34 th annual \n",
      "ham\n",
      "135 : Subject: aggie virus\n",
      "as we don\n",
      "ham\n",
      "136 : Subject: gpg windows 2000 / ou\n",
      "ham\n",
      "137 : Subject: re : so cal edison cl\n",
      "ham\n",
      "138 : Subject: broad update on el pa\n",
      "ham\n",
      "139 : Subject: this will make you fe\n",
      "ham\n",
      "140 : Subject: intranet consolidatio\n",
      "ham\n",
      "141 : Subject: eog agreement\n",
      "here is\n",
      "ham\n",
      "142 : Subject: bullets 9 / 8\n",
      "team - \n",
      "ham\n",
      "143 : Subject: egyptian festival\n",
      ">\n",
      "h\n",
      "ham\n",
      "144 : Subject: eia ' s latest foreca\n",
      "ham\n",
      "145 : Subject: chairman ' s award no\n",
      "ham\n",
      "146 : Subject: it wouldn ' t be enro\n",
      "ham\n",
      "147 : Subject: temporary labor sourc\n",
      "ham\n",
      "148 : Subject: certificate status re\n",
      "ham\n",
      "149 : Subject: rofr pricing\n",
      "lorriain\n",
      "ham\n",
      "150 : Subject: new hire training opp\n",
      "ham\n",
      "151 : Subject: juvenile diabetes fou\n",
      "ham\n",
      "152 : Subject: quiz !\n",
      "q . what do re\n",
      "ham\n",
      "153 : Subject: re : contracts withou\n",
      "ham\n",
      "154 : Subject: customer list\n",
      "please \n",
      "ham\n",
      "155 : Subject: re : eog pronghorn lo\n",
      "ham\n",
      "156 : Subject: re : eog pronghorn lo\n",
      "ham\n",
      "157 : Subject: pipeline map\n",
      "michelle\n",
      "ham\n",
      "158 : Subject: re : eog pronghorn lo\n",
      "ham\n",
      "159 : Subject: christmas list\n",
      "i ' ve\n",
      "ham\n",
      "160 : Subject: chairman ' s award no\n",
      "ham\n",
      "161 : Subject: holiday schedule 2001\n",
      "ham\n",
      "162 : Subject: el paso to sell oasis\n",
      "ham\n",
      "163 : Subject: taking up the slack -\n",
      "ham\n",
      "164 : Subject: cera conference call \n",
      "ham\n",
      "165 : Subject: open season\n",
      "open seas\n",
      "ham\n",
      "166 : Subject: reorganization of hou\n",
      "ham\n",
      "167 : Subject: kern river announces \n",
      "ham\n",
      "168 : Subject: new poi on tw / calpi\n",
      "ham\n",
      "169 : Subject: urgent - time change \n",
      "ham\n",
      "170 : Subject: cera conference call \n",
      "ham\n",
      "171 : Subject: anonymous reporting f\n",
      "ham\n",
      "172 : Subject: open enrollment 2001 \n",
      "ham\n",
      "173 : Subject: fw : no subject\n",
      "fyi\n",
      "-\n",
      "ham\n",
      "174 : Subject: more details on kern \n",
      "ham\n",
      "175 : Subject: december course offer\n",
      "ham\n",
      "176 : Subject: midmonth report : ear\n",
      "ham\n",
      "177 : Subject: allegheny energy\n",
      "more\n",
      "ham\n",
      "178 : Subject: over 7 . 5 bcf / d se\n",
      "ham\n",
      "179 : Subject: transwestern open sea\n",
      "ham\n",
      "180 : Subject: open season notices\n",
      "a\n",
      "ham\n",
      "181 : Subject: latest enron business\n",
      "ham\n",
      "182 : Subject: update on lodi and wh\n",
      "ham\n",
      "183 : Subject: transwestern weekly r\n",
      "ham\n",
      "184 : Subject: tw weekly 11 - 17 - 0\n",
      "ham\n",
      "185 : Subject: tw expansion\n",
      "e prime \n",
      "ham\n",
      "186 : Subject: monday staff meeting\n",
      "\n",
      "ham\n",
      "187 : Subject: open season results\n",
      "a\n",
      "ham\n",
      "188 : Subject: enron in action 11 . \n",
      "ham\n",
      "189 : Subject: new eog well\n",
      "fyi , ke\n",
      "ham\n",
      "190 : Subject: california market bri\n",
      "ham\n",
      "191 : Subject: delayed sticker shock\n",
      "ham\n",
      "192 : Subject: order on tw options\n",
      "i\n",
      "ham\n",
      "193 : Subject: enron / ets code of e\n",
      "ham\n",
      "194 : Subject: holiday safety remind\n",
      "ham\n",
      "195 : Subject: edison mission energy\n",
      "ham\n",
      "196 : Subject: open season results\n",
      "w\n",
      "ham\n",
      "197 : Subject: edison mission energy\n",
      "ham\n",
      "198 : Subject: transwestern weekly r\n",
      "ham\n",
      "199 : Subject: tw weekly 11 / 22 / 0\n",
      "ham\n",
      "200 : Subject: november pricing call\n",
      "ham\n",
      "201 : Subject: bad proposed decision\n",
      "ham\n",
      "202 : Subject: revised phone list at\n",
      "ham\n",
      "203 : Subject: more info on higher g\n",
      "ham\n",
      "204 : Subject: christmas cards going\n",
      "ham\n",
      "205 : Subject: 3 maintenance notes\n",
      "i\n",
      "ham\n",
      "206 : Subject: complimentary issue o\n",
      "ham\n",
      "207 : Subject: enron in action 11 . \n",
      "ham\n",
      "208 : Subject: tw ' s christmas lunc\n",
      "ham\n",
      "209 : Subject: kern river to do anot\n",
      "ham\n",
      "210 : Subject: proposed decision\n",
      "the\n",
      "ham\n",
      "211 : Subject: executive customer li\n",
      "ham\n",
      "212 : Subject: transportation notes \n",
      "ham\n",
      "213 : Subject: iraq ' s antisanction\n",
      "ham\n",
      "214 : Subject: re : park n ride\n",
      "lind\n",
      "ham\n",
      "215 : Subject: california gas prices\n",
      "ham\n",
      "216 : Subject: re : shipper imbalanc\n",
      "ham\n",
      "217 : Subject: article on skilling '\n",
      "ham\n",
      "218 : Subject: must read : isc . enr\n",
      "ham\n",
      "219 : Subject: nov , 2000 pnr billin\n",
      "ham\n",
      "220 : Subject: tw bullets 12 / 1\n",
      "pip\n",
      "ham\n",
      "221 : Subject: project checklist\n",
      "eff\n",
      "ham\n",
      "222 : Subject: re : revised supply /\n",
      "ham\n",
      "223 : Subject: see jeff skilling ' s\n",
      "ham\n",
      "224 : Subject: fw : your music\n",
      "liste\n",
      "ham\n",
      "225 : Subject: re : movie suggestion\n",
      "ham\n",
      "226 : Subject: wallpaper\n",
      "just open t\n",
      "ham\n",
      "227 : Subject: re : download basis d\n",
      "ham\n",
      "228 : Subject: operator choice forms\n",
      "ham\n",
      "229 : Subject: new market / supply a\n",
      "ham\n",
      "230 : Subject: holiday raffle in enr\n",
      "ham\n",
      "231 : Subject: pulse survey memo fro\n",
      "ham\n",
      "232 : Subject: tw ebb re - posting\n",
      "c\n",
      "ham\n",
      "233 : Subject: market / supply list\n",
      "\n",
      "ham\n",
      "234 : Subject: cera insight : the ga\n",
      "ham\n",
      "235 : Subject: enron in action 12 . \n",
      "ham\n",
      "236 : Subject: ets on the move . . .\n",
      "ham\n",
      "237 : Subject: ngi ' s transwestern \n",
      "ham\n",
      "238 : Subject: joe , please add the \n",
      "ham\n",
      "239 : Subject: transwestern weekly r\n",
      "ham\n",
      "240 : Subject: tw bullets 12 / 8\n",
      "tra\n",
      "ham\n",
      "241 : Subject: tw weekly 12 - 8 - 00\n",
      "ham\n",
      "242 : Subject: enron in action 12 . \n",
      "ham\n",
      "243 : Subject: certificate status re\n",
      "ham\n",
      "244 : Subject: team meeting\n",
      "we ' ll \n",
      "ham\n",
      "245 : Subject: the storm arrives - c\n",
      "ham\n",
      "246 : Subject: western market crisis\n",
      "ham\n",
      "247 : Subject: update on ammonia shu\n",
      "ham\n",
      "248 : Subject: weather alert\n",
      "the pot\n",
      "ham\n",
      "249 : Subject: email / voice mail re\n",
      "ham\n",
      "250 : Subject: tw imbalances 12 / 11\n",
      "ham\n",
      "251 : Subject: marketlink and questa\n",
      "ham\n",
      "252 : Subject: alliance may test the\n",
      "ham\n",
      "253 : Subject: happy holidays !\n",
      "with\n",
      "ham\n",
      "254 : Subject: srrs decommissioning \n",
      "ham\n",
      "255 : Subject: california on the bri\n",
      "ham\n",
      "256 : Subject: el paso maintenance\n",
      "e\n",
      "ham\n",
      "257 : Subject: tw deal analysis - ba\n",
      "ham\n",
      "258 : Subject: transwestern weekly t\n",
      "ham\n",
      "259 : Subject: enron in action 12 . \n",
      "ham\n",
      "260 : Subject: ferc restructures cal\n",
      "ham\n",
      "261 : Subject: enron named great pla\n",
      "ham\n",
      "262 : Subject: stratospheric levels \n",
      "ham\n",
      "263 : Subject: maybe tw should build\n",
      "ham\n",
      "264 : Subject: pogo checklist - revi\n",
      "ham\n",
      "265 : Subject: pogo interconnect lea\n",
      "ham\n",
      "266 : Subject: el paso maintenance n\n",
      "ham\n",
      "267 : Subject: re : hi !\n",
      "not too bad\n",
      "ham\n",
      "268 : Subject: re : hi !\n",
      "the \" party\n",
      "ham\n",
      "269 : Subject: ets security requests\n",
      "ham\n",
      "270 : Subject: tw bullets 12 / 22\n",
      "ca\n",
      "ham\n",
      "271 : Subject: c . a . focus meeting\n",
      "ham\n",
      "272 : Subject: calendar\n",
      "here ' s the\n",
      "ham\n",
      "273 : Subject: transportation contra\n",
      "ham\n",
      "274 : Subject: matching gifts\n",
      "enron \n",
      "ham\n",
      "275 : Subject: if you arrange admini\n",
      "ham\n",
      "276 : Subject: preparing and commmun\n",
      "ham\n",
      "277 : Subject: nesa / hea 2001 techn\n",
      "ham\n",
      "278 : Subject: instructions for life\n",
      "ham\n",
      "279 : Subject: update on el paso ' s\n",
      "ham\n",
      "280 : Subject: available capacity\n",
      "mo\n",
      "ham\n",
      "281 : Subject: tw 2001 weekend on - \n",
      "ham\n",
      "282 : Subject: tw park and ride bill\n",
      "ham\n",
      "283 : Subject: = ? ansi _ x 3 . 4 - \n",
      "ham\n",
      "284 : Subject: organizational announ\n",
      "ham\n",
      "285 : Subject: hot line request\n",
      "joe \n",
      "ham\n",
      "286 : Subject: january 4 th daily up\n",
      "ham\n",
      "287 : Subject: contract admin proced\n",
      "ham\n",
      "288 : Subject: ena org changes\n",
      "enron\n",
      "ham\n",
      "289 : Subject: winter driving\n",
      "please\n",
      "ham\n",
      "290 : Subject: eol webtext\n",
      "here is m\n",
      "ham\n",
      "291 : Subject: eol docs\n",
      "michelle : h\n",
      "ham\n",
      "292 : Subject: tw weekly 1 / 5 / 01\n",
      "\n",
      "ham\n",
      "293 : Subject: tw sale / hedge of un\n",
      "ham\n",
      "294 : Subject: ethink about it : 1 /\n",
      "ham\n",
      "295 : Subject: follow - up to prc em\n",
      "ham\n",
      "296 : Subject: enron in action 01 . \n",
      "ham\n",
      "297 : Subject: erequest\n",
      "attached is \n",
      "ham\n",
      "298 : Subject: re : tw security acce\n",
      "ham\n",
      "299 : Subject: training rooms - conf\n",
      "ham\n",
      "300 : Subject: el paso asked to expa\n",
      "ham\n",
      "301 : Subject: technical training - \n",
      "ham\n",
      "302 : Subject: top 10 new year ' s r\n",
      "ham\n",
      "303 : Subject: martin luther king , \n",
      "ham\n",
      "304 : Subject: january 10 th daily u\n",
      "ham\n",
      "305 : Subject: flu shots\n",
      "flu shots c\n",
      "ham\n",
      "306 : Subject: early news on the cal\n",
      "ham\n",
      "307 : Subject: borsheim ' s web site\n",
      "ham\n",
      "308 : Subject: wininstall will run o\n",
      "ham\n",
      "309 : Subject: january 11 th update\n",
      "\n",
      "ham\n",
      "310 : Subject: re : revised language\n",
      "ham\n",
      "311 : Subject: re : keyex customer i\n",
      "ham\n",
      "312 : Subject: tw bullets 1 / 12\n",
      "tra\n",
      "ham\n",
      "313 : Subject: a colossal and danger\n",
      "ham\n",
      "314 : Subject: fyi - el paso\n",
      "el paso\n",
      "ham\n",
      "315 : Subject: el paso maintenance -\n",
      "ham\n",
      "316 : Subject: outlook demonstration\n",
      "ham\n",
      "317 : Subject: fw : outlook demonstr\n",
      "ham\n",
      "318 : Subject: january 12 th update\n",
      "\n",
      "ham\n",
      "319 : Subject: enron onsite childcar\n",
      "ham\n",
      "320 : Subject: enron in action 01 . \n",
      "ham\n",
      "321 : Subject: ethink about it : 01 \n",
      "ham\n",
      "322 : Subject: enron sells hpl and b\n",
      "ham\n",
      "323 : Subject: houston energy expo d\n",
      "ham\n",
      "324 : Subject: notice ! !\n",
      "all the ma\n",
      "ham\n",
      "325 : Subject: january 15 th update\n",
      "\n",
      "ham\n",
      "326 : Subject: january 16 th update\n",
      "\n",
      "ham\n",
      "327 : Subject: transwestern transpor\n",
      "ham\n",
      "328 : Subject: california update\n",
      "ca \n",
      "ham\n",
      "329 : Subject: re : round table meet\n",
      "ham\n",
      "330 : Subject: fw : monthly gas brie\n",
      "ham\n",
      "331 : Subject: membership mixer tomo\n",
      "ham\n",
      "332 : Subject: nesa / hea personal f\n",
      "ham\n",
      "333 : Subject: erequest password\n",
      "you\n",
      "ham\n",
      "334 : Subject: request submitted : a\n",
      "ham\n",
      "335 : Subject: outlook migration det\n",
      "ham\n",
      "336 : Subject: supervisory leadershi\n",
      "ham\n",
      "337 : Subject: ski trip\n",
      "( see attach\n",
      "ham\n",
      "338 : Subject: gas transportation\n",
      "da\n",
      "ham\n",
      "339 : Subject: vacation carryover\n",
      "wa\n",
      "ham\n",
      "340 : Subject: relief for earthquake\n",
      "ham\n",
      "341 : Subject: calif governor declar\n",
      "ham\n",
      "342 : Subject: el paso responds to f\n",
      "ham\n",
      "343 : Subject: passport signup\n",
      ">\n",
      "her\n",
      "ham\n",
      "344 : Subject: successful working re\n",
      "ham\n",
      "345 : Subject: tw weekly 1 / 19 / 01\n",
      "ham\n",
      "346 : Subject: tw bullets 1 / 19\n",
      "cap\n",
      "ham\n",
      "347 : Subject: re : elpaso / tw eddy\n",
      "ham\n",
      "348 : Subject: re : pogo interconnec\n",
      "ham\n",
      "349 : Subject: agreement with eol\n",
      "ju\n",
      "ham\n",
      "350 : Subject: nng capacity books tr\n",
      "ham\n",
      "351 : Subject: fw : high speed regul\n",
      "ham\n",
      "352 : Subject: ethink about it : 01 \n",
      "ham\n",
      "353 : Subject: news from etv\n",
      "it ' s \n",
      "ham\n",
      "354 : Subject: esource presents lexi\n",
      "ham\n",
      "355 : Subject: enron bids farewell t\n",
      "ham\n",
      "356 : Subject: pulse survey results\n",
      "\n",
      "ham\n",
      "357 : Subject: pulse survey results\n",
      "\n",
      "ham\n",
      "358 : Subject: technical training wi\n",
      "ham\n",
      "359 : Subject: re : an additional mo\n",
      "ham\n",
      "360 : Subject: re : i love you ! ! !\n",
      "ham\n",
      "361 : Subject: re : an additional mo\n",
      "ham\n",
      "362 : Subject: january 23 rd update\n",
      "\n",
      "ham\n",
      "363 : Subject: tickets\n",
      "you don ' t h\n",
      "ham\n",
      "364 : Subject: outlook demonstration\n",
      "ham\n",
      "365 : Subject: re : california capac\n",
      "ham\n",
      "366 : Subject: expansion\n",
      "attached pl\n",
      "ham\n",
      "367 : Subject: re : gas transportati\n",
      "ham\n",
      "368 : Subject: january report / febr\n",
      "ham\n",
      "369 : Subject: i ' m sure you guys h\n",
      "ham\n",
      "370 : Subject: eol customer list\n",
      "mic\n",
      "ham\n",
      "371 : Subject: eol customer list\n",
      "mic\n",
      "ham\n",
      "372 : Subject: january 25 th update\n",
      "\n",
      "ham\n",
      "373 : Subject: we beat the street ! \n",
      "ham\n",
      "374 : Subject: cera conf call on the\n",
      "ham\n",
      "375 : Subject: tw lft system enhance\n",
      "ham\n",
      "376 : Subject: tw bullets 1 / 26\n",
      "cap\n",
      "ham\n",
      "377 : Subject: daily ft\n",
      "daily firm c\n",
      "ham\n",
      "378 : Subject: daily ft\n",
      "daily firm c\n",
      "ham\n",
      "379 : Subject: tw bullets 1 / 26\n",
      "cap\n",
      "ham\n",
      "380 : Subject: january 26 th update\n",
      "\n",
      "ham\n",
      "381 : Subject: kick - off meeting fo\n",
      "ham\n",
      "382 : Subject: ethink about it : 01 \n",
      "ham\n",
      "383 : Subject: esource presents free\n",
      "ham\n",
      "384 : Subject: enron transportation \n",
      "ham\n",
      "385 : Subject: results of calif powe\n",
      "ham\n",
      "386 : Subject: fw : western energy m\n",
      "ham\n",
      "387 : Subject: webcast of 2001 inves\n",
      "ham\n",
      "388 : Subject: re : eol letter\n",
      "poste\n",
      "ham\n",
      "389 : Subject: re : tw customer lett\n",
      "ham\n",
      "390 : Subject: sempra ( daily firm )\n",
      "ham\n",
      "391 : Subject: january 29 th update\n",
      "\n",
      "ham\n",
      "392 : Subject: re : tw customer lett\n",
      "ham\n",
      "393 : Subject: re : tw customer lett\n",
      "ham\n",
      "394 : Subject: re : eol letter\n",
      "poste\n",
      "ham\n",
      "395 : Subject: tw parknride procedur\n",
      "ham\n",
      "396 : Subject: = ? ansi _ x 3 . 4 - \n",
      "ham\n",
      "397 : Subject: india relief effort\n",
      "o\n",
      "ham\n",
      "398 : Subject: sdwt task force\n",
      "the s\n",
      "ham\n",
      "399 : Subject: tw transportation con\n",
      "ham\n",
      "400 : Subject: tw transportation con\n",
      "ham\n",
      "401 : Subject: january 30 th update\n",
      "\n",
      "ham\n",
      "402 : Subject: re : pogo interconnec\n",
      "ham\n",
      "403 : Subject: eim organization chan\n",
      "ham\n",
      "404 : Subject: meningitis outbreak &\n",
      "ham\n",
      "405 : Subject: cal - px shuts down t\n",
      "ham\n",
      "406 : Subject: additional bid criter\n",
      "ham\n",
      "407 : Subject: re : pogo interconnec\n",
      "ham\n",
      "408 : Subject: january 31 st update\n",
      "\n",
      "ham\n",
      "409 : Subject: greg whalley floor me\n",
      "ham\n",
      "410 : Subject: learn technical analy\n",
      "ham\n",
      "411 : Subject: offshore museum tour \n",
      "ham\n",
      "412 : Subject: lindy ' s b - day\n",
      "hi \n",
      "ham\n",
      "413 : Subject: red rock - newark gro\n",
      "ham\n",
      "414 : Subject: organization announce\n",
      "ham\n",
      "415 : Subject: eol summary\n",
      "the eol p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n",
      "416 : Subject: thank you ets !\n",
      "thank\n",
      "ham\n",
      "417 : Subject: united way\n",
      "ets proved\n",
      "ham\n",
      "418 : Subject: thank you for your co\n",
      "ham\n",
      "419 : Subject: i didn ' t realize th\n",
      "ham\n",
      "420 : Subject: re : tw capacity\n",
      "can \n",
      "ham\n",
      "421 : Subject: tw capacity\n",
      "michelle \n",
      "ham\n",
      "422 : Subject: re : sun devil expans\n",
      "ham\n",
      "423 : Subject: tw 637 meeting\n",
      "please\n",
      "ham\n",
      "424 : Subject: fw : wt leg issues\n",
      "he\n",
      "ham\n",
      "425 : Subject: eog material\n",
      "i unders\n",
      "ham\n",
      "426 : Subject: organizational announ\n",
      "ham\n",
      "427 : Subject: home address\n",
      "i am wor\n",
      "ham\n",
      "428 : Subject: see you monday , augu\n",
      "ham\n",
      "429 : Subject: here is the estimate \n",
      "ham\n",
      "430 : Subject: tw firm contract proc\n",
      "ham\n",
      "431 : Subject: etc - event - thanksg\n",
      "ham\n",
      "432 : Subject: etc - 2001 museum of \n",
      "ham\n",
      "433 : Subject: planning weekly repor\n",
      "ham\n",
      "434 : Subject: fw : pg & e withdrawa\n",
      "ham\n",
      "435 : Subject: sun devil team introd\n",
      "ham\n",
      "436 : Subject: fw : tw 637 meeting\n",
      "a\n",
      "ham\n",
      "437 : Subject: new tw poi 78161\n",
      "this\n",
      "ham\n",
      "438 : Subject: 8 / 22 cec meeting - \n",
      "ham\n",
      "439 : Subject: = ? ansi _ x 3 . 4 - \n",
      "ham\n",
      "440 : Subject: socal unbundling\n",
      "at t\n",
      "ham\n",
      "441 : Subject: new risk management p\n",
      "ham\n",
      "442 : Subject: thank you ets\n",
      "thank y\n",
      "ham\n",
      "443 : Subject: fw : california capac\n",
      "ham\n",
      "444 : Subject: tw commercial weekly \n",
      "ham\n",
      "445 : Subject: planning weekly repor\n",
      "ham\n",
      "446 : Subject: etc - event - space c\n",
      "ham\n",
      "447 : Subject: re : sj pipe settling\n",
      "ham\n",
      "448 : Subject: replay of the employe\n",
      "ham\n",
      "449 : Subject: ken ' s back !\n",
      "read t\n",
      "ham\n",
      "450 : Subject: fw : interconnect agr\n",
      "ham\n",
      "451 : Subject: september transportat\n",
      "ham\n",
      "452 : Subject: fw : endangered speci\n",
      "ham\n",
      "453 : Subject: contract assignment h\n",
      "ham\n",
      "454 : Subject: contract # 27496 for \n",
      "ham\n",
      "455 : Subject: fw : tw fts agreement\n",
      "ham\n",
      "456 : Subject: dave neubauer on espe\n",
      "ham\n",
      "457 : Subject: an invitation for nes\n",
      "ham\n",
      "458 : Subject: fw : el paso had mark\n",
      "ham\n",
      "459 : Subject: fw : tw mdq\n",
      "- - - - -\n",
      "ham\n",
      "460 : Subject: etc - event - schlitt\n",
      "ham\n",
      "461 : Subject: fw : test your southe\n",
      "ham\n",
      "462 : Subject: tw pnr billing - nove\n",
      "ham\n",
      "463 : Subject: response to proposal \n",
      "ham\n",
      "464 : Subject: response to proposal \n",
      "ham\n",
      "465 : Subject: red rock contracts im\n",
      "ham\n",
      "466 : Subject: re : transwestern res\n",
      "ham\n",
      "467 : Subject: attention remote acce\n",
      "ham\n",
      "468 : Subject: nymex site\n",
      "mark mccon\n",
      "ham\n",
      "469 : Subject: i / b link capacity f\n",
      "ham\n",
      "470 : Subject: i / b link space\n",
      "per \n",
      "ham\n",
      "471 : Subject: updated q & as for en\n",
      "ham\n",
      "472 : Subject: attention : downtown \n",
      "ham\n",
      "473 : Subject: tw pnr activity thru \n",
      "ham\n",
      "474 : Subject: socal unbundling\n",
      "at y\n",
      "ham\n",
      "475 : Subject: re : socal unbundling\n",
      "ham\n",
      "476 : Subject: el paso puts assets u\n",
      "ham\n",
      "477 : Subject: tw imbalance summary\n",
      "\n",
      "ham\n",
      "478 : Subject: new schedule for bid \n",
      "ham\n",
      "479 : Subject: ngi article on trailb\n",
      "ham\n",
      "480 : Subject: american express stat\n",
      "ham\n",
      "481 : Subject: re : united way\n",
      "we ' \n",
      "ham\n",
      "482 : Subject: fw : tw station 2 uni\n",
      "ham\n",
      "483 : Subject: california capacity r\n",
      "ham\n",
      "484 : Subject: attention : changes i\n",
      "ham\n",
      "485 : Subject: transportation\n",
      "michel\n",
      "ham\n",
      "486 : Subject: tw imbalances\n",
      "sharon\n",
      "\n",
      "ham\n",
      "487 : Subject: nesa ' s nymex brown \n",
      "ham\n",
      "488 : Subject: tw pnr activity for d\n",
      "ham\n",
      "489 : Subject: rofr capacity\n",
      "susan ,\n",
      "ham\n",
      "490 : Subject: fw : ena ctrc ' s\n",
      "fyi\n",
      "ham\n",
      "491 : Subject: fw : computers\n",
      "happy \n",
      "ham\n",
      "492 : Subject: new transwestern poi \n",
      "ham\n",
      "493 : Subject: correction : tw pnr f\n",
      "ham\n",
      "494 : Subject: contracts / ple on ca\n",
      "ham\n",
      "495 : Subject: fw : pg peters , jerr\n",
      "ham\n",
      "496 : Subject: ena capacity release\n",
      "\n",
      "ham\n",
      "497 : Subject: metro bus passes and \n",
      "ham\n",
      "498 : Subject: contract # 27496\n",
      "per \n",
      "ham\n",
      "499 : Subject: tw transport\n",
      "please i\n",
      "ham\n",
      "500 : Subject: enervest ( k 24568 ) \n",
      "ham\n",
      "501 : Subject: virginia power pnr\n",
      "mi\n",
      "ham\n",
      "502 : Subject: 2002 on call schedule\n",
      "ham\n",
      "503 : Subject: dynegydirect : new tr\n",
      "ham\n",
      "504 : Subject: t , h : eyeforenergy \n",
      "ham\n",
      "505 : Subject: energy derivatives bo\n",
      "ham\n",
      "506 : Subject: from cindy olson , co\n",
      "ham\n",
      "507 : Subject: set up\n",
      "michelle :\n",
      "ok \n",
      "ham\n",
      "508 : Subject: fw : pg & e informati\n",
      "ham\n",
      "509 : Subject: tw global settlement \n",
      "ham\n",
      "510 : Subject: ets pipeline integrit\n",
      "ham\n",
      "511 : Subject: rodeo tickets\n",
      "below i\n",
      "ham\n",
      "512 : Subject: pnm terminates merger\n",
      "ham\n",
      "513 : Subject: gas storage conferenc\n",
      "ham\n",
      "514 : Subject: etc - rodeo / carniva\n",
      "ham\n",
      "515 : Subject: bus pass and parking \n",
      "ham\n",
      "516 : Subject: can ' t please everyo\n",
      "ham\n",
      "517 : Subject: re : transwestern cap\n",
      "ham\n",
      "518 : Subject: fw : you are invited \n",
      "ham\n",
      "519 : Subject: re : devon sfs\n",
      "that w\n",
      "ham\n",
      "520 : Subject: fw : transwestern cap\n",
      "ham\n",
      "521 : Subject: fw : california capac\n",
      "ham\n",
      "522 : Subject: fw : weather sites\n",
      "th\n",
      "ham\n",
      "523 : Subject: re : procedures for t\n",
      "ham\n",
      "524 : Subject: northeast spring memb\n",
      "ham\n",
      "525 : Subject: interesting\n",
      "look what\n",
      "ham\n",
      "526 : Subject: electric and gas indu\n",
      "ham\n",
      "527 : Subject: us transmission repor\n",
      "ham\n",
      "528 : Subject: kase and company deri\n",
      "ham\n",
      "529 : Subject: transport\n",
      "hey are you\n",
      "ham\n",
      "530 : Subject: tw top ten - browncov\n",
      "ham\n",
      "531 : Subject: h : eyeforenergy brie\n",
      "ham\n",
      "532 : Subject: tw weekly report for \n",
      "ham\n",
      "533 : Subject: it just keeps getting\n",
      "ham\n",
      "534 : Subject: transwestern schedule\n",
      "ham\n",
      "535 : Subject: message from stan hor\n",
      "ham\n",
      "536 : Subject: credit union changes\n",
      "\n",
      "ham\n",
      "537 : Subject: transcripts of steve \n",
      "ham\n",
      "538 : Subject: derivatives class apr\n",
      "ham\n",
      "539 : Subject: the desk - - free sam\n",
      "ham\n",
      "540 : Subject: dynegydirect product \n",
      "ham\n",
      "541 : Subject: transmission study no\n",
      "ham\n",
      "542 : Subject: climate prediction ce\n",
      "ham\n",
      "543 : Subject: re : scope : pg & e l\n",
      "ham\n",
      "544 : Subject: california capacity r\n",
      "ham\n",
      "545 : Subject: steve cooper voicemai\n",
      "ham\n",
      "546 : Subject: dynegydirect maintena\n",
      "ham\n",
      "547 : Subject: presidents ' day holi\n",
      "ham\n",
      "548 : Subject: it is with sincere sa\n",
      "ham\n",
      "549 : Subject: new pgs training soft\n",
      "ham\n",
      "550 : Subject: fw : article\n",
      "- - - - \n",
      "ham\n",
      "551 : Subject: february 2002 schedul\n",
      "ham\n",
      "552 : Subject: red rock delays\n",
      "the l\n",
      "ham\n",
      "553 : Subject: re : distribution lis\n",
      "ham\n",
      "554 : Subject: fw : tw marketing dat\n",
      "ham\n",
      "555 : Subject: ppl energyplus , llc\n",
      "\n",
      "ham\n",
      "556 : Subject: tw weekly report for \n",
      "ham\n",
      "557 : Subject: mojave schedule 9 / 0\n",
      "ham\n",
      "558 : Subject: mojave schedule vols\n",
      "\n",
      "ham\n",
      "559 : Subject: fw : gas daily from p\n",
      "ham\n",
      "560 : Subject: ferc 101 / 102 : marc\n",
      "ham\n",
      "561 : Subject: project finance week \n",
      "ham\n",
      "562 : Subject: fw : on the first day\n",
      "ham\n",
      "563 : Subject: northeast spring memb\n",
      "ham\n",
      "564 : Subject: nesa ' s upcoming eve\n",
      "ham\n",
      "565 : Subject: one more . . .\n",
      "http :\n",
      "ham\n",
      "566 : Subject: special delivery from\n",
      "ham\n",
      "567 : Subject: houston / philadelphi\n",
      "ham\n",
      "568 : Subject: transwestern capacity\n",
      "ham\n",
      "569 : Subject: transwestern capacity\n",
      "ham\n",
      "570 : Subject: learn technical analy\n",
      "ham\n",
      "571 : Subject: t , h : eyeforenergy \n",
      "ham\n",
      "572 : Subject: tw imbalance summary\n",
      "\n",
      "ham\n",
      "573 : Subject: next wave of energy t\n",
      "ham\n",
      "574 : Subject: fwd : nwp system noti\n",
      "ham\n",
      "575 : Subject: tw weekly report for \n",
      "ham\n",
      "576 : Subject: new power executive -\n",
      "ham\n",
      "577 : Subject: tw weekend scheduled \n",
      "ham\n",
      "578 : Subject: re : tw cashouts\n",
      "hi m\n",
      "ham\n",
      "579 : Subject: call when you receive\n",
      "ham\n",
      "580 : Subject: oba and cashout agree\n",
      "ham\n",
      "581 : Subject: fw : confirmation\n",
      "- -\n",
      "ham\n",
      "582 : Subject: burlington oba\n",
      "please\n",
      "ham\n",
      "583 : Subject: management changes\n",
      "as\n",
      "ham\n",
      "584 : Subject: contract # 27600 ( ba\n",
      "ham\n",
      "585 : Subject: fw : training\n",
      "i guess\n",
      "ham\n",
      "586 : Subject: market price volatili\n",
      "ham\n",
      "587 : Subject: tw weekly report for \n",
      "ham\n",
      "588 : Subject: steve cooper voicemai\n",
      "ham\n",
      "589 : Subject: organization meeting\n",
      "\n",
      "ham\n",
      "590 : Subject: marketing\n",
      "michelle : \n",
      "ham\n",
      "591 : Subject: fw :\n",
      "- - - - - origin\n",
      "ham\n",
      "592 : Subject: new power executive -\n",
      "ham\n",
      "593 : Subject: north american gas st\n",
      "ham\n",
      "594 : Subject: decommission wireless\n",
      "ham\n",
      "595 : Subject: additional strips in \n",
      "ham\n",
      "596 : Subject: tw weekend scheduled \n",
      "ham\n",
      "597 : Subject: fw : re ivanhoe e . s\n",
      "ham\n",
      "598 : Subject: fw : abandoned pipe o\n",
      "ham\n",
      "599 : Subject: fw : tw question in a\n",
      "ham\n",
      "600 : Subject: fw : re ivanhoe e . s\n",
      "ham\n",
      "601 : Subject: advs\n",
      "greetings ,\n",
      "i am\n",
      "spam\n",
      "602 : Subject: whats new in summer ?\n",
      "spam\n",
      "603 : Subject: \n",
      "h $ ello\n",
      "dea 54 r ho\n",
      "spam\n",
      "604 : Subject: : ) ) you can not sav\n",
      "spam\n",
      "605 : Subject: need software ? click\n",
      "spam\n",
      "606 : Subject: slotting order confir\n",
      "spam\n",
      "607 : Subject: we shiip to ur countr\n",
      "spam\n",
      "608 : Subject: dicine site on the ne\n",
      "spam\n",
      "609 : Subject: take the reins\n",
      "become\n",
      "spam\n",
      "610 : Subject: today\n",
      "hey ,\n",
      "last week\n",
      "spam\n",
      "611 : Subject: important news for us\n",
      "spam\n",
      "612 : Subject: super - discounts on \n",
      "spam\n",
      "613 : Subject: saiba tudo sobre sua \n",
      "spam\n",
      "614 : Subject: free adware removal s\n",
      "spam\n",
      "615 : Subject: claim your winning of\n",
      "spam\n",
      "616 : Subject: winning notification\n",
      "\n",
      "spam\n",
      "617 : Subject: last week\n",
      "hey ,\n",
      "last \n",
      "spam\n",
      "618 : Subject: 6 - refinance today a\n",
      "spam\n",
      "619 : Subject: urgent investor alert\n",
      "spam\n",
      "620 : Subject: winning notification\n",
      "\n",
      "spam\n",
      "621 : Subject: attract the opposite \n",
      "spam\n",
      "622 : Subject: re : howdy there - ch\n",
      "spam\n",
      "623 : Subject: hey you totally forgo\n",
      "spam\n",
      "624 : Subject: lowes . t pri . ces f\n",
      "spam\n",
      "625 : Subject: salsa , merengue , ba\n",
      "spam\n",
      "626 : Subject: h ~ ow to get er ` ec\n",
      "spam\n",
      "627 : Subject: i sure wish you weren\n",
      "spam\n",
      "628 : Subject: now you can be more p\n",
      "spam\n",
      "629 : Subject: allergies bothering y\n",
      "spam\n",
      "630 : Subject: award notification ! \n",
      "spam\n",
      "631 : Subject: winning notification \n",
      "spam\n",
      "632 : Subject: get all the microsoft\n",
      "spam\n",
      "633 : Subject: hi\n",
      "here\n",
      "spam\n",
      "634 : Subject: re : become a ministe\n",
      "spam\n",
      "635 : Subject: re : congratulations\n",
      "\n",
      "spam\n",
      "636 : Subject: its shannon dr 3 ov 5\n",
      "spam\n",
      "637 : Subject: urgentt ransaction . \n",
      "spam\n",
      "638 : Subject: hey , no husband at h\n",
      "spam\n",
      "639 : Subject: confirm your applicat\n",
      "spam\n",
      "640 : Subject: re [ 3 ]\n",
      "assian it ' \n",
      "spam\n",
      "641 : Subject: la mejor capacitación\n",
      "spam\n",
      "642 : Subject: have a nice weekend .\n",
      "spam\n",
      "643 : Subject: fortune award winning\n",
      "spam\n",
      "644 : Subject: the health chanel : l\n",
      "spam\n",
      "645 : Subject: fashion boys for ever\n",
      "spam\n",
      "646 : Subject: b e s t â â s 0 f t w\n",
      "spam\n",
      "647 : Subject: xãnax for less\n",
      "sa ; v\n",
      "spam\n",
      "648 : Subject: buy regalis , also kn\n",
      "spam\n",
      "649 : Subject: printer ink report - \n",
      "spam\n",
      "650 : Subject: vicodin * hydracodone\n",
      "spam\n",
      "651 : Subject: drive them crazy with\n",
      "spam\n",
      "652 : Subject: make $ 472\n",
      "hello ,\n",
      "we\n",
      "spam\n",
      "653 : Subject: f y i\n",
      "finally\n",
      "one ' s\n",
      "spam\n",
      "654 : Subject: b , uy vi ` codin . o\n",
      "spam\n",
      "655 : Subject: from 10 but\n",
      "janis abe\n",
      "spam\n",
      "656 : Subject: to holland ' s shop c\n",
      "spam\n",
      "657 : Subject: congratulations ! ! !\n",
      "spam\n",
      "658 : Subject: from mr . john\n",
      "john n\n",
      "spam\n",
      "659 : Subject: we are practically gi\n",
      "spam\n",
      "660 : Subject: re : barney\n",
      "u n i v e\n",
      "spam\n",
      "661 : Subject: 45 - ud . . . ballyho\n",
      "spam\n",
      "662 : Subject: litigation relief wit\n",
      "spam\n",
      "663 : Subject: work some magic\n",
      "our\n",
      "p\n",
      "spam\n",
      "664 : Subject: be in control . . . n\n",
      "spam\n",
      "665 : Subject: $ 81122\n",
      "hi ,\n",
      "we sent \n",
      "spam\n",
      "666 : Subject: e ` n ` 1 ` a ` r ` g\n",
      "spam\n",
      "667 : Subject: \n",
      "up to 70 % savings o\n",
      "spam\n",
      "668 : Subject: cheep vlagra through \n",
      "spam\n",
      "669 : Subject: very urgent\n",
      "from : mr\n",
      "spam\n",
      "670 : Subject: a playmate , celebrit\n",
      "spam\n",
      "671 : Subject: labial sa \" ve money \n",
      "spam\n",
      "672 : Subject: find low - cost softw\n",
      "spam\n",
      "673 : Subject: cheapsoft support new\n",
      "spam\n",
      "674 : Subject: vãlium and x . anax s\n",
      "spam\n",
      "675 : Subject: experience more power\n",
      "spam\n",
      "676 : Subject: fwd : work\n",
      "hence perf\n",
      "spam\n",
      "677 : Subject: are you serious , mot\n",
      "spam\n",
      "678 : Subject: hello from micheal ! \n",
      "spam\n",
      "679 : Subject: here is your new pass\n",
      "spam\n",
      "680 : Subject: want your medication \n",
      "spam\n",
      "681 : Subject: offer . bweakfk\n",
      "super\n",
      "spam\n",
      "682 : Subject: congratulations ! ? ?\n",
      "spam\n",
      "683 : Subject: buy darvon , xnax , v\n",
      "spam\n",
      "684 : Subject: want your medication \n",
      "spam\n",
      "685 : Subject: swamp 3 bubbles\n",
      "email\n",
      "spam\n",
      "686 : Subject: \n",
      "câmera segurança sim\n",
      "spam\n",
      "687 : Subject: repelled , america ' \n",
      "spam\n",
      "688 : Subject: = ? windows - 1252 ? \n",
      "spam\n",
      "689 : Subject: re : section finished\n",
      "spam\n",
      "690 : Subject: fwd : got meds ? we h\n",
      "spam\n",
      "691 : Subject: titles from autocad a\n",
      "spam\n",
      "692 : Subject: new soft\n",
      "new qem soft\n",
      "spam\n",
      "693 : Subject: damon fwd ; vi\n",
      "himsel\n",
      "spam\n",
      "694 : Subject: vãlium and x . anax s\n",
      "spam\n",
      "695 : Subject: lose 19 % , powerful \n",
      "spam\n",
      "696 : Subject: re : swollen ear ? . \n",
      "spam\n",
      "697 : Subject: notification\n",
      "finally \n",
      "spam\n",
      "698 : Subject: need honest associate\n",
      "spam\n",
      "699 : Subject: fwd : great news\n",
      "dear\n",
      "spam\n",
      "700 : Subject: the final application\n",
      "spam\n",
      "701 : Subject: yo , u ar @ e exp ^ o\n",
      "spam\n",
      "702 : Subject: sa . ve up to 70 % of\n",
      "spam\n",
      "703 : Subject: \n",
      "hel ' lo tirm 8 ed o\n",
      "spam\n",
      "704 : Subject: xanax $ 95 , \\ / aliu\n",
      "spam\n",
      "705 : Subject: try plexus , the male\n",
      "spam\n",
      "706 : Subject: larges incest collect\n",
      "spam\n",
      "707 : Subject: marketing offer\n",
      "dear \n",
      "spam\n",
      "708 : Subject: paink / illers availa\n",
      "spam\n",
      "709 : Subject: cheating house wife 7\n",
      "spam\n",
      "710 : Subject: re [ 15 ]\n",
      "here ' s yo\n",
      "spam\n",
      "711 : Subject: online ordering is th\n",
      "spam\n",
      "712 : Subject: no marketing . no pro\n",
      "spam\n",
      "713 : Subject: arturo how could you \n",
      "spam\n",
      "714 : Subject: winning notification \n",
      "spam\n",
      "715 : Subject: yours sinserelly\n",
      "barr\n",
      "spam\n",
      "716 : Subject: re : 61 % - off \\ / i\n",
      "spam\n",
      "717 : Subject: from dr ndeye\n",
      "from th\n",
      "spam\n",
      "718 : Subject: alyssa 25 has invited\n",
      "spam\n",
      "719 : Subject: 75 % off for all new \n",
      "spam\n",
      "720 : Subject: fwd : [ student ] 74 \n",
      "spam\n",
      "721 : Subject: affordable online pre\n",
      "spam\n",
      "722 : Subject: ( no subject )\n",
      "hello \n",
      "spam\n",
      "723 : Subject: our little secret\n",
      "sa \n",
      "spam\n",
      "724 : Subject: to bonilla ' s shop c\n",
      "spam\n",
      "725 : Subject: hard to find items fo\n",
      "spam\n",
      "726 : Subject: the fountain of youth\n",
      "spam\n",
      "727 : Subject: you need only 15 minu\n",
      "spam\n",
      "728 : Subject: cheap online prescrip\n",
      "spam\n",
      "729 : Subject: = ? iso - 8859 - 1 ? \n",
      "spam\n",
      "730 : Subject: harriett how could yo\n",
      "spam\n",
      "731 : Subject: fw : [ belt ] 64 % of\n",
      "spam\n",
      "732 : Subject: appointment change fo\n",
      "spam\n",
      "733 : Subject: buy xãnax now .\n",
      "sa ; \n",
      "spam\n",
      "734 : Subject: \n",
      "hey\n",
      "reflnance now an\n",
      "spam\n",
      "735 : Subject: sa ' ve mon , ey\n",
      "hi\n",
      "w\n",
      "spam\n",
      "736 : Subject: here is your new pass\n",
      "spam\n",
      "737 : Subject: stock business\n",
      "from :\n",
      "spam\n",
      "738 : Subject: àãõâÞ üùé · õèêÞã ¨ Ý\n",
      "spam\n",
      "739 : Subject: inexpensive online me\n",
      "spam\n",
      "740 : Subject: \n",
      "the permanent fix to\n",
      "spam\n",
      "741 : Subject: brand name and generi\n",
      "spam\n",
      "742 : Subject: small guys finish las\n",
      "spam\n",
      "743 : Subject: found a better soluti\n",
      "spam\n",
      "744 : Subject: re : account # 51682 \n",
      "spam\n",
      "745 : Subject: best pharma deals\n",
      "sa \n",
      "spam\n",
      "746 : Subject: from : mr . adams fem\n",
      "spam\n",
      "747 : Subject: re : simple at home a\n",
      "spam\n",
      "748 : Subject: = ? utf - 8 ? q ? per\n",
      "spam\n",
      "749 : Subject: vãlium and x . anax s\n",
      "spam\n",
      "750 : Subject: the inches kept comin\n",
      "spam\n",
      "751 : Subject: = ? utf - 8 ? q ? bas\n",
      "spam\n",
      "752 : Subject: bait @ em . ca bolivi\n",
      "spam\n",
      "753 : Subject: new discovery may hel\n",
      "spam\n",
      "754 : Subject: business proposal\n",
      "you\n",
      "spam\n",
      "755 : Subject: xp pro $ 50 , ms offi\n",
      "spam\n",
      "756 : Subject: never pay more than $\n",
      "spam\n",
      "757 : Subject: your best source for \n",
      "spam\n",
      "758 : Subject: refina ` nce an ' d s\n",
      "spam\n",
      "759 : Subject: rates go up in novemb\n",
      "spam\n",
      "760 : Subject: you are a winner ; { \n",
      "spam\n",
      "761 : Subject: cheap online drugs he\n",
      "spam\n",
      "762 : Subject: hi again ,\n",
      "here is ma\n",
      "spam\n",
      "763 : Subject: parcelas en pucon\n",
      "si\n",
      "\n",
      "spam\n",
      "764 : Subject: don ` t be stupid ver\n",
      "spam\n",
      "765 : Subject: finally i got it\n",
      "toda\n",
      "spam\n",
      "766 : Subject: monster cock pornno\n",
      "m\n",
      "spam\n",
      "767 : Subject: greatest online pills\n",
      "spam\n",
      "768 : Subject: greatest online presc\n",
      "spam\n",
      "769 : Subject: sample\n",
      "mortars transi\n",
      "spam\n",
      "770 : Subject: 900 branded watches g\n",
      "spam\n",
      "771 : Subject: let ' s work together\n",
      "spam\n",
      "772 : Subject: = ? utf - 8 ? q ? i c\n",
      "spam\n",
      "773 : Subject: congratulations you h\n",
      "spam\n",
      "774 : Subject: hi again ,\n",
      "here is pa\n",
      "spam\n",
      "775 : Subject: hi again ,\n",
      "here is ev\n",
      "spam\n",
      "776 : Subject: windows xp and office\n",
      "spam\n",
      "777 : Subject: new dating site\n",
      "check\n",
      "spam\n",
      "778 : Subject: bank - erbschaft\n",
      "aus \n",
      "spam\n",
      "779 : Subject: 6 - refinance to 3 . \n",
      "spam\n",
      "780 : Subject: get ready to s \\ ave \n",
      "spam\n",
      "781 : Subject: i need your urgent as\n",
      "spam\n",
      "782 : Subject: urgent contact\n",
      "engr .\n",
      "spam\n",
      "783 : Subject: re : account # 247681\n",
      "spam\n",
      "784 : Subject: premium online medica\n",
      "spam\n",
      "785 : Subject: top quality medicatio\n",
      "spam\n",
      "786 : Subject: xp pro $ 50 , ms offi\n",
      "spam\n",
      "787 : Subject: your application is p\n",
      "spam\n",
      "788 : Subject: yýllýk 25 milyona dom\n",
      "spam\n",
      "789 : Subject: hi again ,\n",
      "here is le\n",
      "spam\n",
      "790 : Subject: purchase steroids onl\n",
      "spam\n",
      "791 : Subject: lyman is the paper re\n",
      "spam\n",
      "792 : Subject: cheapest cial % is , \n",
      "spam\n",
      "793 : Subject: your acceptance in th\n",
      "spam\n",
      "794 : Subject: read and get back to \n",
      "spam\n",
      "795 : Subject: we sell regalis for a\n",
      "spam\n",
      "796 : Subject: joe does this make yo\n",
      "spam\n",
      "797 : Subject: finally the b 3 st pr\n",
      "spam\n",
      "798 : Subject: discount c ! a \" l ! \n",
      "spam\n",
      "799 : Subject: hiv viruses ? check t\n",
      "spam\n",
      "800 : Subject: your r _ x order is r\n",
      "spam\n",
      "801 : Subject: seguro de afianzamien\n",
      "spam\n",
      "802 : Subject: impotence is totally \n",
      "spam\n",
      "803 : Subject: photoshop , windows ,\n",
      "spam\n",
      "804 : Subject: partnership investmen\n",
      "spam\n",
      "805 : Subject: buy vãlium now\n",
      "sa ; v\n",
      "spam\n",
      "806 : Subject: coreldraw $ 100 , ado\n",
      "spam\n",
      "807 : Subject: well ?\n",
      "x files disney\n",
      "spam\n",
      "808 : Subject: coreldraw $ 100 , ado\n",
      "spam\n",
      "809 : Subject: i ' m feeling great n\n",
      "spam\n",
      "810 : Subject: © ú ¬ p ¬ ü » ?å ] ° \n",
      "spam\n",
      "811 : Subject: health news report\n",
      "- \n",
      "spam\n",
      "812 : Subject: be ready for her when\n",
      "spam\n",
      "813 : Subject: see how much more wea\n",
      "spam\n",
      "814 : Subject: burmese dyer conquer\n",
      "\n",
      "spam\n",
      "815 : Subject: look what the cat dra\n",
      "spam\n",
      "816 : Subject: finest online drugs h\n",
      "spam\n",
      "817 : Subject: new ! = ? iso - 8859 \n",
      "spam\n",
      "818 : Subject: re : the oem software\n",
      "spam\n",
      "819 : Subject: how is life ?\n",
      "online\n",
      "\n",
      "spam\n",
      "820 : Subject: something unusual\n",
      "bap\n",
      "spam\n",
      "821 : Subject: reminder\n",
      "psychologist\n",
      "spam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822 : Subject: finest online pills h\n",
      "spam\n",
      "823 : Subject: finest online drugs h\n",
      "spam\n",
      "824 : Subject: sexually - explicit :\n",
      "spam\n",
      "825 : Subject: may i have a moment ?\n",
      "spam\n",
      "826 : Subject: buy regalis , also kn\n",
      "spam\n",
      "827 : Subject: learn what jp morgan \n",
      "spam\n",
      "828 : Subject: xp pro , adobe , offi\n",
      "spam\n",
      "829 : Subject: norton 2004 $ 15 . of\n",
      "spam\n",
      "830 : Subject: affordable online pre\n",
      "spam\n",
      "831 : Subject: here you go .\n",
      "deposit\n",
      "spam\n",
      "832 : Subject: the hottest cams on t\n",
      "spam\n",
      "833 : Subject: make six figures at h\n",
      "spam\n",
      "834 : Subject: popular software at l\n",
      "spam\n",
      "835 : Subject: is it funny ?\n",
      "attaini\n",
      "spam\n",
      "836 : Subject: re : [ innumerable ] \n",
      "spam\n",
      "837 : Subject: xxx dating\n",
      "amatuer ma\n",
      "spam\n",
      "838 : Subject: our pro - forma invoi\n",
      "spam\n",
      "839 : Subject: greatest online medic\n",
      "spam\n",
      "840 : Subject: unbeleivable deals fo\n",
      "spam\n",
      "841 : Subject: one - time amazing jo\n",
      "spam\n",
      "842 : Subject: smoking hot amateurs\n",
      "\n",
      "spam\n",
      "843 : Subject: vicodin ' only $ 150 \n",
      "spam\n",
      "844 : Subject: more and more choices\n",
      "spam\n",
      "845 : Subject: application confirmat\n",
      "spam\n",
      "846 : Subject: new healthcare databa\n",
      "spam\n",
      "847 : Subject: let us fill your p [ \n",
      "spam\n",
      "848 : Subject: a plea for help\n",
      "from \n",
      "spam\n",
      "849 : Subject: she ' s not happy if \n",
      "spam\n",
      "850 : Subject: estrategias avanzadas\n",
      "spam\n",
      "851 : Subject: wating to hear from y\n",
      "spam\n",
      "852 : Subject: legitamate software a\n",
      "spam\n",
      "853 : Subject: horny babes\n",
      "hey dude\n",
      "\n",
      "spam\n",
      "854 : Subject: cheap offshore viagra\n",
      "spam\n",
      "855 : Subject: internet pharmacy\n",
      "sto\n",
      "spam\n",
      "856 : Subject: are you desperate fro\n",
      "spam\n",
      "857 : Subject: live and work in the \n",
      "spam\n",
      "858 : Subject: greatest online drugs\n",
      "spam\n",
      "859 : Subject: * cancer : nuevo trat\n",
      "spam\n",
      "860 : Subject: you can save up to 70\n",
      "spam\n",
      "861 : Subject: new ! vìagra soft tab\n",
      "spam\n",
      "862 : Subject: play on ocean treasur\n",
      "spam\n",
      "863 : Subject: ( none )\n",
      "dear , frien\n",
      "spam\n",
      "864 : Subject: kindest attention\n",
      "kin\n",
      "spam\n",
      "865 : Subject: greatest online medic\n",
      "spam\n",
      "866 : Subject: citibank alerting ser\n",
      "spam\n",
      "867 : Subject: xp pro $ 5 o , adobe \n",
      "spam\n",
      "868 : Subject: brand name and generi\n",
      "spam\n",
      "869 : Subject: warren how could you \n",
      "spam\n",
      "870 : Subject: bruceg , !\n",
      "valiumxana\n",
      "spam\n",
      "871 : Subject: lo invitamos a conoce\n",
      "spam\n",
      "872 : Subject: best software prices \n",
      "spam\n",
      "873 : Subject: reduc ^ e your mor ! \n",
      "spam\n",
      "874 : Subject: greatest online medic\n",
      "spam\n",
      "875 : Subject: cheapest meds you ' l\n",
      "spam\n",
      "876 : Subject: cheap oem software sh\n",
      "spam\n",
      "877 : Subject: super vkiagra\n",
      "generic\n",
      "spam\n",
      "878 : Subject: cheap online pills he\n",
      "spam\n",
      "879 : Subject: your source for onlin\n",
      "spam\n",
      "880 : Subject: nt 4 . 0 terminal ser\n",
      "spam\n",
      "881 : Subject: photoshop 7 + premier\n",
      "spam\n",
      "882 : Subject: we sell regalis for a\n",
      "spam\n",
      "883 : Subject: = ? utf - 8 ? q ? equ\n",
      "spam\n",
      "884 : Subject: \n",
      "when was the last ti\n",
      "spam\n",
      "885 : Subject: cheap online medicati\n",
      "spam\n",
      "886 : Subject: watch and learn about\n",
      "spam\n",
      "887 : Subject: even your appetite is\n",
      "spam\n",
      "888 : Subject: office xp $ 100 , nor\n",
      "spam\n",
      "889 : Subject: penls enlargement pll\n",
      "spam\n",
      "890 : Subject: = ? utf - 8 ? q ? equ\n",
      "spam\n",
      "891 : Subject: enlarge your bre 4 st\n",
      "spam\n",
      "892 : Subject: inexpensive online pr\n",
      "spam\n",
      "893 : Subject: top quality drugs her\n",
      "spam\n",
      "894 : Subject: brand name and generi\n",
      "spam\n",
      "895 : Subject: brand name and generi\n",
      "spam\n",
      "896 : Subject: cheap online pills he\n",
      "spam\n",
      "897 : Subject: newsweek : u . s visa\n",
      "spam\n",
      "898 : Subject: 5 oo - 35 oo per day \n",
      "spam\n",
      "899 : Subject: = ? utf - 8 ? q ? man\n",
      "spam\n",
      "900 : Subject: seminario asexma de t\n",
      "spam\n",
      "901 : Subject: congratulation ! ! !\n",
      "\n",
      "spam\n",
      "902 : Subject: liquidation de la sem\n",
      "spam\n",
      "903 : Subject: seminario reduccion d\n",
      "spam\n",
      "904 : Subject: alex , action will re\n",
      "spam\n",
      "905 : Subject: do you care ?\n",
      "rationa\n",
      "spam\n",
      "906 : Subject: = ? utf - 8 ? q ? equ\n",
      "spam\n",
      "907 : Subject: xãnax here\n",
      "sa ; ve 7 \n",
      "spam\n",
      "908 : Subject: [ ãê ° ­ ãß ] ¡ á ¡ á\n",
      "spam\n",
      "909 : Subject: this really helped me\n",
      "spam\n",
      "910 : Subject: strong buy > > otc : \n",
      "spam\n",
      "911 : Subject: get your pres $ cript\n",
      "spam\n",
      "912 : Subject: = ? utf - 8 ? q ? xma\n",
      "spam\n",
      "913 : Subject: cheap online medicati\n",
      "spam\n",
      "914 : Subject: half price windows 20\n",
      "spam\n",
      "915 : Subject: vãlium here\n",
      "sa ; ve 7\n",
      "spam\n",
      "916 : Subject: home delivery cia ` l\n",
      "spam\n",
      "917 : Subject: sexual frustrated ? ?\n",
      "spam\n",
      "918 : Subject: pre - approved applic\n",
      "spam\n",
      "919 : Subject: reduce stress\n",
      "re - fi\n",
      "spam\n",
      "920 : Subject: get nominated for mba\n",
      "spam\n",
      "921 : Subject: cheap online drugs he\n",
      "spam\n",
      "922 : Subject: re : account # 9505 p\n",
      "spam\n",
      "923 : Subject: windows xp and office\n",
      "spam\n",
      "924 : Subject: madge , absence of pr\n",
      "spam\n",
      "925 : Subject: photos\n",
      "glade bismark \n",
      "spam\n",
      "926 : Subject: wonderful phamraceuti\n",
      "spam\n",
      "927 : Subject: cheap online prescrip\n",
      "spam\n",
      "928 : Subject: same quailty , much l\n",
      "spam\n",
      "929 : Subject: experience the  safe\n",
      "spam\n",
      "930 : Subject: you ' ve heard it all\n",
      "spam\n",
      "931 : Subject: littlestocks sizzle s\n",
      "spam\n",
      "932 : Subject: brand name and generi\n",
      "spam\n",
      "933 : Subject: syria\n",
      "you ' re invite\n",
      "spam\n",
      "934 : Subject: partnership in busine\n",
      "spam\n",
      "935 : Subject: welcome to www . card\n",
      "spam\n",
      "936 : Subject: no - more exercise\n",
      "at\n",
      "spam\n",
      "937 : Subject: i am so happy\n",
      "\" do i \n",
      "spam\n",
      "938 : Subject: cheap online tablets \n",
      "spam\n",
      "939 : Subject: top quality medicatio\n",
      "spam\n",
      "940 : Subject: greatest online medic\n",
      "spam\n",
      "941 : Subject: finest online pills h\n",
      "spam\n",
      "942 : Subject: ch 3 ap xanaks pil 1 \n",
      "spam\n",
      "943 : Subject: news alert > > gdvi a\n",
      "spam\n",
      "944 : Subject: cheapest meds you ' l\n",
      "spam\n",
      "945 : Subject: looking for cheap hig\n",
      "spam\n",
      "946 : Subject: about celebration\n",
      "inn\n",
      "spam\n",
      "947 : Subject: med alert\n",
      "link to com\n",
      "spam\n",
      "948 : Subject: marylou , the religio\n",
      "spam\n",
      "949 : Subject: dlscount clalis\n",
      "new c\n",
      "spam\n",
      "950 : Subject: new product ! cialis \n",
      "spam\n",
      "951 : Subject: re : account # 2160 d\n",
      "spam\n",
      "952 : Subject: specialize in hard to\n",
      "spam\n",
      "953 : Subject: shrivelwynn belies\n",
      "ch\n",
      "spam\n",
      "954 : Subject: your attention\n",
      "from :\n",
      "spam\n",
      "955 : Subject: cvs get vicodin here \n",
      "spam\n",
      "956 : Subject: = ? utf - 8 ? q ? any\n",
      "spam\n",
      "957 : Subject: xp pro $ 5 o . adobe \n",
      "spam\n",
      "958 : Subject: = ? utf - 8 ? q ? man\n",
      "spam\n",
      "959 : Subject: re : our offer\n",
      "i like\n",
      "spam\n",
      "960 : Subject: congratulations\n",
      "bank \n",
      "spam\n",
      "961 : Subject: do you want a watch ?\n",
      "spam\n",
      "962 : Subject: take it to the bank s\n",
      "spam\n",
      "963 : Subject: gosse , there are no \n",
      "spam\n",
      "964 : Subject: i need your psycholog\n",
      "spam\n",
      "965 : Subject: bruceg impress your f\n",
      "spam\n",
      "966 : Subject: partner with me\n",
      "urgen\n",
      "spam\n",
      "967 : Subject: software at incredibl\n",
      "spam\n",
      "968 : Subject: congratulation , winn\n",
      "spam\n",
      "969 : Subject: re your pharmacy orde\n",
      "spam\n",
      "970 : Subject: i have this mouth pro\n",
      "spam\n",
      "971 : Subject: 77 % off for all new \n",
      "spam\n",
      "972 : Subject: i recommend this earl\n",
      "spam\n",
      "973 : Subject: best online drugs her\n",
      "spam\n",
      "974 : Subject: best online tablets h\n",
      "spam\n",
      "975 : Subject: next of kin [ busines\n",
      "spam\n",
      "976 : Subject: bu ' y vicdoin at hom\n",
      "spam\n",
      "977 : Subject: m , eds is now 50 % o\n",
      "spam\n",
      "978 : Subject: fwd : great news\n",
      "it i\n",
      "spam\n",
      "979 : Subject: co - operation\n",
      "hello \n",
      "spam\n",
      "980 : Subject: please solve your imp\n",
      "spam\n",
      "981 : Subject: watch and learn about\n",
      "spam\n",
      "982 : Subject: vi . codin , is here \n",
      "spam\n",
      "983 : Subject: suffering depression \n",
      "spam\n",
      "984 : Subject: help is on the way di\n",
      "spam\n",
      "985 : Subject: need ambien or other \n",
      "spam\n",
      "986 : Subject: no more paying for mo\n",
      "spam\n",
      "987 : Subject: you are nominated for\n",
      "spam\n",
      "988 : Subject: \n",
      "avoid paying top dol\n",
      "spam\n",
      "989 : Subject: re : draftsmen\n",
      "doge p\n",
      "spam\n",
      "990 : Subject: re : account # 41826 \n",
      "spam\n",
      "991 : Subject: agua fria y caliente \n",
      "spam\n",
      "992 : Subject: 404 plan on wednesday\n",
      "spam\n",
      "993 : Subject: buy vãlium now\n",
      "sa ; v\n",
      "spam\n",
      "994 : Subject: re @ your pharmacy o \n",
      "spam\n",
      "995 : Subject: metallurgist\n",
      "email lo\n",
      "spam\n",
      "996 : Subject: you ' ve been selecte\n",
      "spam\n",
      "997 : Subject: from the office of au\n",
      "spam\n",
      "998 : Subject: hiv / aids award winn\n",
      "spam\n",
      "999 : Subject: buy vãlium and x . an\n",
      "spam\n",
      "1000 : Subject: = ? utf - 8 ? q ? all\n",
      "spam\n",
      "1001 : Subject: great news\n",
      "very well \n",
      "spam\n",
      "1002 : Subject: xãnax for less\n",
      "sa ; v\n",
      "spam\n",
      "1003 : Subject: want to own your own \n",
      "spam\n",
      "1004 : Subject: critical spyware warn\n",
      "spam\n",
      "1005 : Subject: = ? utf - 8 ? q ? sav\n",
      "spam\n",
      "1006 : Subject: greetings\n",
      "mr . joseph\n",
      "spam\n",
      "1007 : Subject: computer software for\n",
      "spam\n",
      "1008 : Subject: = ? utf - 8 ? q ? enj\n",
      "spam\n",
      "1009 : Subject: cheap online drugs he\n",
      "spam\n",
      "1010 : Subject: inexpensive online me\n",
      "spam\n",
      "1011 : Subject: medicaldirectory , ph\n",
      "spam\n",
      "1012 : Subject: 65 % off for all new \n",
      "spam\n",
      "1013 : Subject: your urgent response \n",
      "spam\n",
      "1014 : Subject: fwd : [ graspable ] 7\n",
      "spam\n",
      "1015 : Subject: your application conf\n",
      "spam\n",
      "1016 : Subject: cheap online pills he\n",
      "spam\n",
      "1017 : Subject: top quality prescript\n",
      "spam\n",
      "1018 : Subject: hello dear member : )\n",
      "spam\n",
      "1019 : Subject: greatest online table\n",
      "spam\n",
      "1020 : Subject: adobe photoshop $ 8 o\n",
      "spam\n",
      "1021 : Subject: you opinion counts\n",
      "ac\n",
      "spam\n",
      "1022 : Subject: confidential business\n",
      "spam\n",
      "1023 : Subject: improve desire charse\n",
      "spam\n",
      "1024 : Subject: enlarge girth and len\n",
      "spam\n",
      "1025 : Subject: give her something to\n",
      "spam\n",
      "1026 : Subject: a perfect stock in an\n",
      "spam\n",
      "1027 : Subject: new breed of equity t\n",
      "spam\n",
      "1028 : Subject: meet ppl in your area\n",
      "spam\n",
      "1029 : Subject: can you last 36 hours\n",
      "spam\n",
      "1030 : Subject: penis enhancement pat\n",
      "spam\n",
      "1031 : Subject: armenian genocide pla\n",
      "spam\n",
      "1032 : Subject: sa : fe - the 100 % h\n",
      "spam\n",
      "1033 : Subject: you too can have mult\n",
      "spam\n",
      "1034 : Subject: 80 % discount on all \n",
      "spam\n",
      "1035 : Subject: from mr li le song\n",
      "fr\n",
      "spam\n",
      "1036 : Subject: der neue kontaktanzei\n",
      "spam\n",
      "1037 : Subject: tock market opportuni\n",
      "spam\n",
      "1038 : Subject: congratulations you h\n",
      "spam\n",
      "1039 : Subject: digital cable filter\n",
      "\n",
      "spam\n",
      "1040 : Subject: you would hate yourse\n",
      "spam\n",
      "1041 : Subject: stallone , just 5 min\n",
      "spam\n",
      "1042 : Subject: pharmaceutical servic\n",
      "spam\n",
      "1043 : Subject: healthy reproductive \n",
      "spam\n",
      "1044 : Subject: find it here\n",
      "your wom\n",
      "spam\n",
      "1045 : Subject: award notification / \n",
      "spam\n",
      "1046 : Subject: final winning notific\n",
      "spam\n",
      "1047 : Subject: re : loonger\n",
      "hello , \n",
      "spam\n",
      "1048 : Subject: hi\n",
      "wife evening least\n",
      "spam\n",
      "1049 : Subject: remember the old days\n",
      "spam\n",
      "1050 : Subject: unleash the animal in\n",
      "spam\n",
      "1051 : Subject: important medical ann\n",
      "spam\n",
      "1052 : Subject: how does the lowest r\n",
      "spam\n",
      "1053 : Subject: marketing service\n",
      "to \n",
      "spam\n",
      "1054 : Subject: important ebay messag\n",
      "spam\n",
      "1055 : Subject: you can eat whatever \n",
      "spam\n",
      "1056 : Subject: we have $ 231 , 165 f\n",
      "spam\n",
      "1057 : Subject: do not walk behind me\n",
      "spam\n",
      "1058 : Subject: award - winning spywa\n",
      "spam\n",
      "1059 : Subject: try downloading faste\n",
      "spam\n",
      "1060 : Subject: review our site for w\n",
      "spam\n",
      "1061 : Subject: wanna download faster\n",
      "spam\n",
      "1062 : Subject: discover what it ' s \n",
      "spam\n",
      "1063 : Subject: get it up again\n",
      "hi th\n",
      "spam\n",
      "1064 : Subject: llego el super cd de \n",
      "spam\n",
      "1065 : Subject: part - time job for e\n",
      "spam\n",
      "1066 : Subject: listas free ! ! ! hoy\n",
      "spam\n",
      "1067 : Subject: via - ggra is lousy e\n",
      "spam\n",
      "1068 : Subject: three steps to the so\n",
      "spam\n",
      "1069 : Subject: why not stop by ?\n",
      "all\n",
      "spam\n",
      "1070 : Subject: their waiting for you\n",
      "spam\n",
      "1071 : Subject: agriculture - transla\n",
      "spam\n",
      "1072 : Subject: best regards\n",
      "dear fri\n",
      "spam\n",
      "1073 : Subject: please read\n",
      "lncrease \n",
      "spam\n",
      "1074 : Subject: re : you neeed our su\n",
      "spam\n",
      "1075 : Subject: giga medzz\n",
      "dear sir /\n",
      "spam\n",
      "1076 : Subject: what car will you buy\n",
      "spam\n",
      "1077 : Subject: tsunami disaster . ( \n",
      "spam\n",
      "1078 : Subject: amazing mortgages for\n",
      "spam\n",
      "1079 : Subject: it savedd my life\n",
      "dea\n",
      "spam\n",
      "1080 : Subject: get a cable filter\n",
      "ho\n",
      "spam\n",
      "1081 : Subject: re : sterling stock w\n",
      "spam\n",
      "1082 : Subject: increase your penis s\n",
      "spam\n",
      "1083 : Subject: all that you wanted t\n",
      "spam\n",
      "1084 : Subject: \n",
      "sud euler gangling p\n",
      "spam\n",
      "1085 : Subject: here you are\n",
      "increase\n",
      "spam\n",
      "1086 : Subject: fw : at our medzplace\n",
      "spam\n",
      "1087 : Subject: vvould you like to se\n",
      "spam\n",
      "1088 : Subject: it ' s not a joke\n",
      "pro\n",
      "spam\n",
      "1089 : Subject: signups with your bus\n",
      "spam\n",
      "1090 : Subject: billing reciept for o\n",
      "spam\n",
      "1091 : Subject: photoshop , windows ,\n",
      "spam\n",
      "1092 : Subject: if you like to pay le\n",
      "spam\n",
      "1093 : Subject: men ' s health\n",
      "to who\n",
      "spam\n",
      "1094 : Subject: this is a good chance\n",
      "spam\n",
      "1095 : Subject: try vi : agra today\n",
      "r\n",
      "spam\n",
      "1096 : Subject: dsj - douro s joao mu\n",
      "spam\n",
      "1097 : Subject: contact me\n",
      "attn ;\n",
      "sir\n",
      "spam\n",
      "1098 : Subject: meet ppl in your area\n",
      "spam\n",
      "1099 : Subject: rates are low bait - \n",
      "spam\n",
      "1100 : Subject: i ' m a changed man\n",
      "d\n",
      "spam\n",
      "1101 : Subject: account info is neede\n",
      "spam\n",
      "1102 : Subject: achieve more with thi\n",
      "spam\n",
      "1103 : Subject: software at incredibl\n",
      "spam\n",
      "1104 : Subject: text\n",
      "strontium alchem\n",
      "spam\n",
      "1105 : Subject: are you man enough fo\n",
      "spam\n",
      "1106 : Subject: clalis $ 2 . 29\n",
      "disco\n",
      "spam\n",
      "1107 : Subject: fyi\n",
      "this is our final\n",
      "spam\n",
      "1108 : Subject: we understand your cr\n",
      "spam\n",
      "1109 : Subject: legal way to increase\n",
      "spam\n",
      "1110 : Subject: get a rate\n",
      "hello ,\n",
      "we\n",
      "spam\n",
      "1111 : Subject: reeally works excelle\n",
      "spam\n",
      "1112 : Subject: saludos desde caracas\n",
      "spam\n",
      "1113 : Subject: claim your lucky winn\n",
      "spam\n",
      "1114 : Subject: moore good medz\n",
      "hello\n",
      "spam\n",
      "1115 : Subject: data you can use beat\n",
      "spam\n",
      "1116 : Subject: 100 % f - r - e - e a\n",
      "spam\n",
      "1117 : Subject: from mary koko\n",
      "humbly\n",
      "spam\n",
      "1118 : Subject: eextra news\n",
      "hello , w\n",
      "spam\n",
      "1119 : Subject: award winning notific\n",
      "spam\n",
      "1120 : Subject: tn you will win your \n",
      "spam\n",
      "1121 : Subject: costco on our dime ba\n",
      "spam\n",
      "1122 : Subject: it ' s not a joke\n",
      "on \n",
      "spam\n",
      "1123 : Subject: very urgent\n",
      "dear sir \n",
      "spam\n",
      "1124 : Subject: select eshopping for \n",
      "spam\n",
      "1125 : Subject: ge of the criminal go\n",
      "spam\n",
      "1126 : Subject: royalization eighteen\n",
      "spam\n",
      "1127 : Subject: become a sexual ghuru\n",
      "spam\n",
      "1128 : Subject: paintball sports equi\n",
      "spam\n",
      "1129 : Subject: it saved my lifee\n",
      "hel\n",
      "spam\n",
      "1130 : Subject: you ' ve won . we owe\n",
      "spam\n",
      "1131 : Subject: do you really wanna p\n",
      "spam\n",
      "1132 : Subject: do you really wanna p\n",
      "spam\n",
      "1133 : Subject: multiple o ' gazm 4 m\n",
      "spam\n",
      "1134 : Subject: save your money by ge\n",
      "spam\n",
      "1135 : Subject: reality television au\n",
      "spam\n",
      "1136 : Subject: blessings to you and \n",
      "spam\n",
      "1137 : Subject: astounding home loans\n",
      "spam\n",
      "1138 : Subject: pass this on to your \n",
      "spam\n",
      "1139 : Subject: new product ! cialis \n",
      "spam\n",
      "1140 : Subject: boost your business\n",
      "d\n",
      "spam\n",
      "1141 : Subject: software now going at\n",
      "spam\n",
      "1142 : Subject: re [ 0 ] : don ' t ge\n",
      "spam\n",
      "1143 : Subject: any software just for\n",
      "spam\n",
      "1144 : Subject: want a cablefilter ?\n",
      "\n",
      "spam\n",
      "1145 : Subject: please read : newslet\n",
      "spam\n",
      "1146 : Subject: congratulations ! ! !\n",
      "spam\n",
      "1147 : Subject: read a message - more\n",
      "spam\n",
      "1148 : Subject: confidence is back\n",
      "he\n",
      "spam\n",
      "1149 : Subject: ' sexually explicit '\n",
      "spam\n",
      "1150 : Subject: you are our lucky win\n",
      "spam\n",
      "1151 : Subject: your partner will wor\n",
      "spam\n",
      "1152 : Subject: don ' t buy viia - gr\n",
      "spam\n",
      "1153 : Subject: very little to get al\n",
      "spam\n",
      "1154 : Subject: gain the erection , l\n",
      "spam\n",
      "1155 : Subject: goood work\n",
      "hello , we\n",
      "spam\n",
      "1156 : Subject: re : [ 5 ]\n",
      "this is go\n",
      "spam\n",
      "1157 : Subject: for your information\n",
      "\n",
      "spam\n",
      "1158 : Subject: we are now open felix\n",
      "spam\n",
      "1159 : Subject: impotence treatment\n",
      "h\n",
      "spam\n",
      "1160 : Subject: c _ i _ a _ l _ i _ s\n",
      "spam\n",
      "1161 : Subject: are you curious how m\n",
      "spam\n",
      "1162 : Subject: extending refinances \n",
      "spam\n",
      "1163 : Subject: extending opportunity\n",
      "spam\n",
      "1164 : Subject: important to you - ma\n",
      "spam\n",
      "1165 : Subject: is great offrr\n",
      "hello \n",
      "spam\n",
      "1166 : Subject: how low can you go\n",
      "de\n",
      "spam\n",
      "1167 : Subject: re : do you own a gro\n",
      "spam\n",
      "1168 : Subject: free adult personals\n",
      "\n",
      "spam\n",
      "1169 : Subject: congratulation you ha\n",
      "spam\n",
      "1170 : Subject: azimuthal swanlike gl\n",
      "spam\n",
      "1171 : Subject: important information\n",
      "spam\n",
      "1172 : Subject: we provide the rxdrug\n",
      "spam\n",
      "1173 : Subject: f / r / e / e cable t\n",
      "spam\n",
      "1174 : Subject: isyour next investmen\n",
      "spam\n",
      "1175 : Subject: = ? windows - 1252 ? \n",
      "spam\n",
      "1176 : Subject: friendly notification\n",
      "spam\n",
      "1177 : Subject: for your information\n",
      "\n",
      "spam\n",
      "1178 : Subject: ciallis softabs onlly\n",
      "spam\n",
      "1179 : Subject: want the sex life to \n",
      "spam\n",
      "1180 : Subject: congratulation ! ! ! \n",
      "spam\n",
      "1181 : Subject: hi sweetness\n",
      "hey babi\n",
      "spam\n",
      "1182 : Subject: you will be favoured \n",
      "spam\n",
      "1183 : Subject: gget rolexes at our c\n",
      "spam\n",
      "1184 : Subject: this house wont burn \n",
      "spam\n",
      "1185 : Subject: has your cum ever dri\n",
      "spam\n",
      "1186 : Subject: 64 % off for all new \n",
      "spam\n",
      "1187 : Subject: invitacion vip\n",
      "si no \n",
      "spam\n",
      "1188 : Subject: best online drvgs her\n",
      "spam\n",
      "1189 : Subject: you can be of help\n",
      "he\n",
      "spam\n",
      "1190 : Subject: consolation prize win\n",
      "spam\n",
      "1191 : Subject: all generic viagra pr\n",
      "spam\n",
      "1192 : Subject: medz forr you\n",
      "hello ,\n",
      "spam\n",
      "1193 : Subject: goodday sir / ma , es\n",
      "spam\n",
      "1194 : Subject: extra time\n",
      "did you ej\n",
      "spam\n",
      "1195 : Subject: miss you\n",
      "heytherecuti\n",
      "spam\n",
      "1196 : Subject: this will change your\n",
      "spam\n",
      "1197 : Subject: customer recognition \n",
      "spam\n",
      "1198 : Subject: hi\n",
      "slow happened end \n",
      "spam\n",
      "1199 : Subject: bait - excelled @ em \n",
      "spam\n",
      "1200 : Subject: great 2 nd branded el\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "BASE_DIR = '/home/aqts/yangHong/first-spam-experiment/'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "#get text email dataset, which is equal to the image email datset\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data/hybrid_email_dataset_equal/text/')\n",
    "print(TEXT_DATA_DIR)\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "nb_filters = 256\n",
    "hiden_lstm_layer = 256\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt'), encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {'ham':0, 'spam':1}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "                        \n",
    "count = 0\n",
    "for folder in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, folder)\n",
    "    for fname in sorted(os.listdir(path)):\n",
    "        if fname is not None:\n",
    "            fpath = os.path.join(path, fname)\n",
    "            args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "            with open(fpath, **args) as f:\n",
    "                t = f.read()\n",
    "                i = t.find('Subject:')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                count+=1\n",
    "                print(count,\":\",t[0:30])\n",
    "            if folder =='ham':\n",
    "                labels.append(0)\n",
    "                print('ham')\n",
    "            else:\n",
    "                labels.append(1)\n",
    "                print('spam')\n",
    "# print('Process text dataset done')\n",
    "# print('count:',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 texts.\n",
      "Found 26464 unique tokens.\n",
      "Shape of data tensor: (1200, 500)\n",
      "Shape of label tensor: (1200, 2)\n",
      "x_train.shape (1200, 500)\n",
      "y_train.shape (1200, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Dropout, GRU   \n",
    "VALIDATION_SPLIT=0.8\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# num_validation_samples = int( VALIDATION_SPLIT* data.shape[0])\n",
    "# test_num=num_validation_samples+int(0.2* data.shape[0])\n",
    "x_text_train = data[:]\n",
    "y_text_train = labels[:]\n",
    "# x_text_valid = data[num_validation_samples:val_num]\n",
    "# y_text_valid = labels[num_validation_samples:val_num]\n",
    "# x_text_test = data[num_validation_samples:]\n",
    "# y_text_test = labels[num_validation_samples:]\n",
    "\n",
    "print('x_train.shape',x_text_train.shape)\n",
    "print('y_train.shape',y_text_train.shape)\n",
    "# print('x_val.shape',x_text_valid.shape)\n",
    "# # print('y_val.shape',y_text_valid.shape)\n",
    "# print('x_test.shape',x_text_test.shape)\n",
    "# print('y_test.shape',y_text_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1200, 128, 128, 3)\n",
      "Shape of labels tensor: (1200, 2)\n",
      "x_train.shape (1200, 128, 128, 3)\n",
      "y_train.shape (1200, 2)\n"
     ]
    }
   ],
   "source": [
    "import keras, cv2,os\n",
    "import numpy as np\n",
    "from PIL import Image,ImageEnhance\n",
    "from keras.utils import to_categorical\n",
    "def read_image(path,imageName):\n",
    "    img=cv2.imread(imageName,cv2.IMREAD_COLOR)\n",
    "#     image=Image.open(imageName)\n",
    "#     contrast = ImageEnhance.Contrast(image)\n",
    "#     img=contrast.enhance(2)\n",
    "#     img = np.asarray(img)\n",
    "    #method1\n",
    "    if img is None:\n",
    "        os.remove(os.path.join(path,imageName))\n",
    "        print(\"remove success\")\n",
    "        return None\n",
    "    else:\n",
    "        img = cv2.resize(img,dsize=(128,128),interpolation=cv2.INTER_LINEAR)\n",
    "        img = img.astype(\"float32\")\n",
    "        img *= (1./255)\n",
    "        b,g,r=cv2.split(img)\n",
    "        img2=cv2.merge([r,g,b])\n",
    "        \n",
    "#         img *= (1./255)\n",
    "#         r, g, b,a = cv2.split(img)\n",
    "#         contrast=cv2.merge([b, g, r])\n",
    "        return img2\n",
    "        \n",
    "def img_processing(path,x,y):\n",
    "    directory =os.listdir(path)\n",
    "    for textPath in directory:\n",
    "        for fn in os.listdir(os.path.join(r\"\",path+ textPath)):\n",
    "#             if fn.endswith('.png') or fn.endswith('.jpg'):\n",
    "            fd = os.path.join(path, textPath, fn)\n",
    "            img_arr=read_image(path,fd)\n",
    "            if(img_arr is not None):\n",
    "                x.append(img_arr)\n",
    "                if(textPath==\"ham\"):\n",
    "                    y.append(0)\n",
    "# #                     img_arr.save(os.path.join(path+\"\\ham\", os.path.basename(img_arr)))\n",
    "#                         pass\n",
    "                else:\n",
    "                    y.append(1)\n",
    "#                     img_arr.save(os.path.join(path+\"\\spam\", os.path.basename(img_arr)))\n",
    "#                         pass\n",
    "    return x,y  \n",
    "x,y=[],[]\n",
    "#get image email dataset, which is equal to the text email datset\n",
    "path1=\"/home/aqts/yangHong/first-spam-experiment/data/hybrid_email_dataset_equal/img/\"\n",
    "\n",
    "\n",
    "x,y=img_processing(path1,x,y)\n",
    "VALIDATION_SPLIT=0.8\n",
    "\n",
    "\n",
    "data = np.array(x)\n",
    "\n",
    "labels = to_categorical(np.asarray(y))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of labels tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# num_validation_samples = int( VALIDATION_SPLIT* data.shape[0])\n",
    "# val_num=num_validation_samples+int(0.2* data.shape[0])\n",
    "x_img_train = data[:]\n",
    "y_img_train = labels[:]\n",
    "# x_img_valid = data[num_validation_samples:val_num]\n",
    "# y_img_valid = labels[num_validation_samples:val_num]\n",
    "# x_img_test = data[num_validation_samples :]\n",
    "# y_img_test = labels[num_validation_samples :]\n",
    "\n",
    "print('x_train.shape',x_img_train.shape)\n",
    "print('y_train.shape',y_img_train.shape)\n",
    "# print('x_valid.shape',x_img_valid.shape)\n",
    "# print('y_valid.shape',y_img_valid.shape)\n",
    "# print('x_test.shape',x_img_test.shape)\n",
    "# print('y_test.shape',y_img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aqts/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/aqts/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/aqts/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aqts/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) \n",
    "img_model = load_model('/home/aqts/yangHong/first-spam-experiment/h5_model/image_cnn_model.h5')\n",
    "text_model = load_model('/home/aqts/yangHong/first-spam-experiment/h5_model/lstm.h5')  \n",
    "# text_model = load_model('lstm_hybrid.h5')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_prob=img_model.predict(x_img_train,batch_size=32, verbose=0)\n",
    "text_prob = text_model.predict(x_text_train,batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 27s 22ms/step\n",
      "\n",
      "acc: 98.42%\n"
     ]
    }
   ],
   "source": [
    "text_scores= text_model.evaluate(x_text_train,y_text_train, batch_size=32)\n",
    "print(\"\\n%s: %.2f%%\" % (\"acc\", text_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 1s 619us/step\n",
      "\n",
      "acc: 95.92%\n"
     ]
    }
   ],
   "source": [
    "img_scores= img_model.evaluate(x_img_train,y_img_train, batch_size=32)\n",
    "print(\"\\n%s: %.2f%%\" % (\"acc\", img_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_prob.shape (1200, 2)\n",
      "text_prob.shape (1200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"img_prob.shape\",img_prob.shape)\n",
    "print(\"text_prob.shape\",text_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 4)\n",
      "(1200, 2)\n"
     ]
    }
   ],
   "source": [
    "hybrid_data=np.concatenate((text_prob,img_prob), axis=1)\n",
    "print(hybrid_data.shape)\n",
    "print(y_text_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aqts/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.6901 - acc: 0.5442\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 0.6317 - acc: 0.6279\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 0.5805 - acc: 0.8996\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 0.5227 - acc: 0.9842\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 185us/step - loss: 0.4539 - acc: 0.9838\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 174us/step - loss: 0.3765 - acc: 0.9833\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 0.3022 - acc: 0.9833\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 182us/step - loss: 0.2386 - acc: 0.9838\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.1887 - acc: 0.9838\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 181us/step - loss: 0.1525 - acc: 0.9838\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 140us/step - loss: 0.1275 - acc: 0.9842\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 158us/step - loss: 0.1104 - acc: 0.9842\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 159us/step - loss: 0.0988 - acc: 0.9838\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.0907 - acc: 0.9842\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 155us/step - loss: 0.0850 - acc: 0.9842\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 163us/step - loss: 0.0810 - acc: 0.9842\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 0.0781 - acc: 0.9842\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 167us/step - loss: 0.0760 - acc: 0.9842\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.0744 - acc: 0.9842\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 153us/step - loss: 0.0732 - acc: 0.9842\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 147us/step - loss: 0.0723 - acc: 0.9842\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 0.0715 - acc: 0.9842\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 167us/step - loss: 0.0709 - acc: 0.9842\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 172us/step - loss: 0.0705 - acc: 0.9838\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 0.0700 - acc: 0.9842\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 147us/step - loss: 0.0697 - acc: 0.9842\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 0.0694 - acc: 0.9838\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 165us/step - loss: 0.0693 - acc: 0.9838\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 173us/step - loss: 0.0691 - acc: 0.9838\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 0.0689 - acc: 0.9842\n",
      "Best: 0.985000 using {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.01, 'optimizer': 'SGD'}\n",
      "0.666667 (0.147558) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.001, 'optimizer': 'SGD'}\n",
      "0.668333 (0.053369) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.001, 'optimizer': 'RMSprop'}\n",
      "0.663333 (0.054365) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.001, 'optimizer': 'Adam'}\n",
      "0.984167 (0.005137) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.01, 'optimizer': 'SGD'}\n",
      "0.984167 (0.005137) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.01, 'optimizer': 'RMSprop'}\n",
      "0.983750 (0.005683) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.01, 'optimizer': 'Adam'}\n",
      "0.984167 (0.006562) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.1, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.1, 'optimizer': 'RMSprop'}\n",
      "0.983750 (0.006374) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.1, 'optimizer': 'Adam'}\n",
      "0.984167 (0.005137) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.2, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.2, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 10, 'learn_rate': 0.2, 'optimizer': 'Adam'}\n",
      "0.869167 (0.166484) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.001, 'optimizer': 'SGD'}\n",
      "0.792083 (0.118279) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.001, 'optimizer': 'RMSprop'}\n",
      "0.946250 (0.058372) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.001, 'optimizer': 'Adam'}\n",
      "0.984167 (0.005137) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.01, 'optimizer': 'SGD'}\n",
      "0.984167 (0.005137) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.01, 'optimizer': 'RMSprop'}\n",
      "0.984167 (0.005137) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.01, 'optimizer': 'Adam'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.1, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.1, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.1, 'optimizer': 'Adam'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.2, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.2, 'optimizer': 'RMSprop'}\n",
      "0.984167 (0.006562) with: {'batch_size': 16, 'epochs': 20, 'learn_rate': 0.2, 'optimizer': 'Adam'}\n",
      "0.949167 (0.041949) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.001, 'optimizer': 'SGD'}\n",
      "0.902083 (0.051683) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.001, 'optimizer': 'RMSprop'}\n",
      "0.902083 (0.062336) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.001, 'optimizer': 'Adam'}\n",
      "0.985000 (0.005401) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.01, 'optimizer': 'SGD'}\n",
      "0.985000 (0.005401) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.01, 'optimizer': 'RMSprop'}\n",
      "0.984167 (0.005137) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.01, 'optimizer': 'Adam'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.1, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.1, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.1, 'optimizer': 'Adam'}\n",
      "0.982500 (0.006124) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.2, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.2, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 16, 'epochs': 30, 'learn_rate': 0.2, 'optimizer': 'Adam'}\n",
      "0.543333 (0.066656) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.001, 'optimizer': 'SGD'}\n",
      "0.772917 (0.064423) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.001, 'optimizer': 'RMSprop'}\n",
      "0.748333 (0.102395) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.001, 'optimizer': 'Adam'}\n",
      "0.983333 (0.005621) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.01, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.01, 'optimizer': 'RMSprop'}\n",
      "0.983750 (0.006374) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.01, 'optimizer': 'Adam'}\n",
      "0.984167 (0.005137) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.1, 'optimizer': 'SGD'}\n",
      "0.985000 (0.005401) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.1, 'optimizer': 'RMSprop'}\n",
      "0.984167 (0.005137) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.1, 'optimizer': 'Adam'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.2, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.2, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 10, 'learn_rate': 0.2, 'optimizer': 'Adam'}\n",
      "0.713750 (0.249393) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.001, 'optimizer': 'SGD'}\n",
      "0.820833 (0.049227) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.001, 'optimizer': 'RMSprop'}\n",
      "0.821250 (0.057509) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.001, 'optimizer': 'Adam'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984167 (0.005137) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.01, 'optimizer': 'SGD'}\n",
      "0.983750 (0.005683) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.01, 'optimizer': 'RMSprop'}\n",
      "0.984167 (0.005137) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.01, 'optimizer': 'Adam'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.1, 'optimizer': 'SGD'}\n",
      "0.983750 (0.005683) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.1, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.1, 'optimizer': 'Adam'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.2, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.2, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 20, 'learn_rate': 0.2, 'optimizer': 'Adam'}\n",
      "0.818750 (0.151145) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.001, 'optimizer': 'SGD'}\n",
      "0.943750 (0.055142) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.001, 'optimizer': 'RMSprop'}\n",
      "0.805000 (0.044616) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.001, 'optimizer': 'Adam'}\n",
      "0.984167 (0.005137) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.01, 'optimizer': 'SGD'}\n",
      "0.984583 (0.005237) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.01, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.005137) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.01, 'optimizer': 'Adam'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.1, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.1, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.1, 'optimizer': 'Adam'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.2, 'optimizer': 'SGD'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.2, 'optimizer': 'RMSprop'}\n",
      "0.983333 (0.006236) with: {'batch_size': 32, 'epochs': 30, 'learn_rate': 0.2, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import merge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import f1_score,recall_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "# hybrid_data_new=np.array(hybrid_data)\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "def create_model(optimizer='adam',learn_rate=0.01):\n",
    "    model = Sequential()  \n",
    "    model.add(Dense(64, input_shape=(4,)))\n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid')) \n",
    "    optimizer = SGD(lr=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy',  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])  \n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# 定义网格搜索参数\n",
    "batch_size = [16,32]\n",
    "epochs = [10,20,30]\n",
    "optimizer = ['SGD', 'RMSprop','Adam']\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs,optimizer=optimizer,learn_rate=learn_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(hybrid_data, y_text_train,verbose=1)\n",
    "# 总结结果\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "960/960 [==============================] - 1s 747us/step - loss: 0.6432 - acc: 0.7448\n",
      "Epoch 2/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.5742 - acc: 0.9818\n",
      "Epoch 3/30\n",
      "960/960 [==============================] - 0s 234us/step - loss: 0.5096 - acc: 0.9823\n",
      "Epoch 4/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.4424 - acc: 0.9823\n",
      "Epoch 5/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.3736 - acc: 0.9828\n",
      "Epoch 6/30\n",
      "960/960 [==============================] - 0s 238us/step - loss: 0.3074 - acc: 0.9828\n",
      "Epoch 7/30\n",
      "960/960 [==============================] - 0s 236us/step - loss: 0.2483 - acc: 0.9828\n",
      "Epoch 8/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.2015 - acc: 0.9833\n",
      "Epoch 9/30\n",
      "960/960 [==============================] - 0s 235us/step - loss: 0.1663 - acc: 0.9833\n",
      "Epoch 10/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.1407 - acc: 0.9833\n",
      "Epoch 11/30\n",
      "960/960 [==============================] - 0s 235us/step - loss: 0.1223 - acc: 0.9833\n",
      "Epoch 12/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.1092 - acc: 0.9828\n",
      "Epoch 13/30\n",
      "960/960 [==============================] - 0s 239us/step - loss: 0.0997 - acc: 0.9833\n",
      "Epoch 14/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.0928 - acc: 0.9828\n",
      "Epoch 15/30\n",
      "960/960 [==============================] - 0s 235us/step - loss: 0.0876 - acc: 0.9833\n",
      "Epoch 16/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.0838 - acc: 0.9833\n",
      "Epoch 17/30\n",
      "960/960 [==============================] - 0s 236us/step - loss: 0.0808 - acc: 0.9833\n",
      "Epoch 18/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0785 - acc: 0.9833\n",
      "Epoch 19/30\n",
      "960/960 [==============================] - 0s 237us/step - loss: 0.0766 - acc: 0.9833\n",
      "Epoch 20/30\n",
      "960/960 [==============================] - 0s 231us/step - loss: 0.0751 - acc: 0.9828\n",
      "Epoch 21/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0739 - acc: 0.9833\n",
      "Epoch 22/30\n",
      "960/960 [==============================] - 0s 238us/step - loss: 0.0729 - acc: 0.9833\n",
      "Epoch 23/30\n",
      "960/960 [==============================] - 0s 237us/step - loss: 0.0720 - acc: 0.9833\n",
      "Epoch 24/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0714 - acc: 0.9828\n",
      "Epoch 25/30\n",
      "960/960 [==============================] - 0s 235us/step - loss: 0.0708 - acc: 0.9833\n",
      "Epoch 26/30\n",
      "960/960 [==============================] - 0s 229us/step - loss: 0.0702 - acc: 0.9833\n",
      "Epoch 27/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.0698 - acc: 0.9833\n",
      "Epoch 28/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0694 - acc: 0.9833\n",
      "Epoch 29/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.0691 - acc: 0.9833\n",
      "Epoch 30/30\n",
      "960/960 [==============================] - 0s 237us/step - loss: 0.0688 - acc: 0.9833\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       110\n",
      "           1       0.99      0.98      0.99       130\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       240\n",
      "   macro avg       0.99      0.99      0.99       240\n",
      "weighted avg       0.99      0.99      0.99       240\n",
      " samples avg       0.99      0.99      0.99       240\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "960/960 [==============================] - 1s 759us/step - loss: 0.6618 - acc: 0.6266\n",
      "Epoch 2/30\n",
      "960/960 [==============================] - 0s 233us/step - loss: 0.6021 - acc: 0.8609\n",
      "Epoch 3/30\n",
      "960/960 [==============================] - 0s 237us/step - loss: 0.5429 - acc: 0.9495\n",
      "Epoch 4/30\n",
      "960/960 [==============================] - 0s 233us/step - loss: 0.4793 - acc: 0.9818\n",
      "Epoch 5/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.4136 - acc: 0.9818\n",
      "Epoch 6/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.3487 - acc: 0.9823\n",
      "Epoch 7/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.2871 - acc: 0.9828\n",
      "Epoch 8/30\n",
      "960/960 [==============================] - 0s 232us/step - loss: 0.2345 - acc: 0.9828\n",
      "Epoch 9/30\n",
      "960/960 [==============================] - 0s 230us/step - loss: 0.1933 - acc: 0.9828\n",
      "Epoch 10/30\n",
      "960/960 [==============================] - 0s 232us/step - loss: 0.1628 - acc: 0.9828\n",
      "Epoch 11/30\n",
      "960/960 [==============================] - 0s 235us/step - loss: 0.1408 - acc: 0.9833\n",
      "Epoch 12/30\n",
      "960/960 [==============================] - 0s 233us/step - loss: 0.1251 - acc: 0.9833\n",
      "Epoch 13/30\n",
      "960/960 [==============================] - 0s 236us/step - loss: 0.1138 - acc: 0.9833\n",
      "Epoch 14/30\n",
      "960/960 [==============================] - 0s 247us/step - loss: 0.1055 - acc: 0.9833\n",
      "Epoch 15/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.0993 - acc: 0.9833\n",
      "Epoch 16/30\n",
      "960/960 [==============================] - 0s 238us/step - loss: 0.0947 - acc: 0.9833\n",
      "Epoch 17/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.0911 - acc: 0.9833\n",
      "Epoch 18/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0884 - acc: 0.9833\n",
      "Epoch 19/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.0863 - acc: 0.9833\n",
      "Epoch 20/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0846 - acc: 0.9833\n",
      "Epoch 21/30\n",
      "960/960 [==============================] - 0s 232us/step - loss: 0.0834 - acc: 0.9833\n",
      "Epoch 22/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.0822 - acc: 0.9833\n",
      "Epoch 23/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0812 - acc: 0.9833\n",
      "Epoch 24/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0805 - acc: 0.9833\n",
      "Epoch 25/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0799 - acc: 0.9833\n",
      "Epoch 26/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0793 - acc: 0.9833\n",
      "Epoch 27/30\n",
      "960/960 [==============================] - 0s 233us/step - loss: 0.0789 - acc: 0.9833\n",
      "Epoch 28/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.0785 - acc: 0.9833\n",
      "Epoch 29/30\n",
      "960/960 [==============================] - 0s 239us/step - loss: 0.0782 - acc: 0.9833\n",
      "Epoch 30/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.0779 - acc: 0.9833\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       123\n",
      "           1       0.97      1.00      0.99       117\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       240\n",
      "   macro avg       0.99      0.99      0.99       240\n",
      "weighted avg       0.99      0.99      0.99       240\n",
      " samples avg       0.99      0.99      0.99       240\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "960/960 [==============================] - 1s 793us/step - loss: 0.6631 - acc: 0.5839\n",
      "Epoch 2/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.6116 - acc: 0.9276\n",
      "Epoch 3/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.5665 - acc: 0.9854\n",
      "Epoch 4/30\n",
      "960/960 [==============================] - 0s 236us/step - loss: 0.5180 - acc: 0.9854\n",
      "Epoch 5/30\n",
      "960/960 [==============================] - 0s 250us/step - loss: 0.4628 - acc: 0.9854\n",
      "Epoch 6/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.3999 - acc: 0.9854\n",
      "Epoch 7/30\n",
      "960/960 [==============================] - 0s 249us/step - loss: 0.3351 - acc: 0.9854\n",
      "Epoch 8/30\n",
      "960/960 [==============================] - 0s 239us/step - loss: 0.2749 - acc: 0.9854\n",
      "Epoch 9/30\n",
      "960/960 [==============================] - 0s 251us/step - loss: 0.2237 - acc: 0.9854\n",
      "Epoch 10/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.1833 - acc: 0.9854\n",
      "Epoch 11/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.1533 - acc: 0.9854\n",
      "Epoch 12/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.1315 - acc: 0.9854\n",
      "Epoch 13/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.1158 - acc: 0.9854\n",
      "Epoch 14/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.1044 - acc: 0.9854\n",
      "Epoch 15/30\n",
      "960/960 [==============================] - 0s 235us/step - loss: 0.0960 - acc: 0.9854\n",
      "Epoch 16/30\n",
      "960/960 [==============================] - 0s 235us/step - loss: 0.0899 - acc: 0.9854\n",
      "Epoch 17/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0852 - acc: 0.9854\n",
      "Epoch 18/30\n",
      "960/960 [==============================] - 0s 231us/step - loss: 0.0817 - acc: 0.9854\n",
      "Epoch 19/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.0789 - acc: 0.9854\n",
      "Epoch 20/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.0767 - acc: 0.9854\n",
      "Epoch 21/30\n",
      "960/960 [==============================] - 0s 246us/step - loss: 0.0749 - acc: 0.9854\n",
      "Epoch 22/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.0735 - acc: 0.9854\n",
      "Epoch 23/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.0723 - acc: 0.9854\n",
      "Epoch 24/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.0713 - acc: 0.9854\n",
      "Epoch 25/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.0706 - acc: 0.9854\n",
      "Epoch 26/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.0698 - acc: 0.9854\n",
      "Epoch 27/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.0693 - acc: 0.9854\n",
      "Epoch 28/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.0688 - acc: 0.9854\n",
      "Epoch 29/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.0683 - acc: 0.9854\n",
      "Epoch 30/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0679 - acc: 0.9854\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       129\n",
      "           1       0.96      1.00      0.98       111\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      " samples avg       0.98      0.98      0.98       240\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "960/960 [==============================] - 1s 845us/step - loss: 0.7151 - acc: 0.5036\n",
      "Epoch 2/30\n",
      "960/960 [==============================] - 0s 238us/step - loss: 0.6684 - acc: 0.6984\n",
      "Epoch 3/30\n",
      "960/960 [==============================] - 0s 236us/step - loss: 0.6286 - acc: 0.7802\n",
      "Epoch 4/30\n",
      "960/960 [==============================] - 0s 239us/step - loss: 0.5872 - acc: 0.9693\n",
      "Epoch 5/30\n",
      "960/960 [==============================] - 0s 236us/step - loss: 0.5368 - acc: 0.9839\n",
      "Epoch 6/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.4792 - acc: 0.9839\n",
      "Epoch 7/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.4167 - acc: 0.9839\n",
      "Epoch 8/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.3529 - acc: 0.9839\n",
      "Epoch 9/30\n",
      "960/960 [==============================] - 0s 247us/step - loss: 0.2924 - acc: 0.9839\n",
      "Epoch 10/30\n",
      "960/960 [==============================] - 0s 237us/step - loss: 0.2398 - acc: 0.9839\n",
      "Epoch 11/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.1978 - acc: 0.9844\n",
      "Epoch 12/30\n",
      "960/960 [==============================] - 0s 233us/step - loss: 0.1656 - acc: 0.9844\n",
      "Epoch 13/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.1418 - acc: 0.9844\n",
      "Epoch 14/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.1245 - acc: 0.9844\n",
      "Epoch 15/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.1117 - acc: 0.9844\n",
      "Epoch 16/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.1023 - acc: 0.9844\n",
      "Epoch 17/30\n",
      "960/960 [==============================] - 0s 231us/step - loss: 0.0953 - acc: 0.9844\n",
      "Epoch 18/30\n",
      "960/960 [==============================] - 0s 236us/step - loss: 0.0899 - acc: 0.9844\n",
      "Epoch 19/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.0858 - acc: 0.9844\n",
      "Epoch 20/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.0826 - acc: 0.9844\n",
      "Epoch 21/30\n",
      "960/960 [==============================] - 0s 247us/step - loss: 0.0801 - acc: 0.9844\n",
      "Epoch 22/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.0780 - acc: 0.9844\n",
      "Epoch 23/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.0763 - acc: 0.9844\n",
      "Epoch 24/30\n",
      "960/960 [==============================] - 0s 251us/step - loss: 0.0750 - acc: 0.9844\n",
      "Epoch 25/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.0738 - acc: 0.9844\n",
      "Epoch 26/30\n",
      "960/960 [==============================] - 0s 228us/step - loss: 0.0729 - acc: 0.9844\n",
      "Epoch 27/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.0721 - acc: 0.9844\n",
      "Epoch 28/30\n",
      "960/960 [==============================] - 0s 253us/step - loss: 0.0715 - acc: 0.9844\n",
      "Epoch 29/30\n",
      "960/960 [==============================] - 0s 239us/step - loss: 0.0709 - acc: 0.9844\n",
      "Epoch 30/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0704 - acc: 0.9844\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       119\n",
      "           1       0.98      0.99      0.98       121\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      " samples avg       0.98      0.98      0.98       240\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "960/960 [==============================] - 1s 879us/step - loss: 0.6819 - acc: 0.5583\n",
      "Epoch 2/30\n",
      "960/960 [==============================] - 0s 240us/step - loss: 0.6405 - acc: 0.6656\n",
      "Epoch 3/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.5999 - acc: 0.9495\n",
      "Epoch 4/30\n",
      "960/960 [==============================] - 0s 226us/step - loss: 0.5555 - acc: 0.9833\n",
      "Epoch 5/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.5033 - acc: 0.9833\n",
      "Epoch 6/30\n",
      "960/960 [==============================] - 0s 246us/step - loss: 0.4430 - acc: 0.9833\n",
      "Epoch 7/30\n",
      "960/960 [==============================] - 0s 249us/step - loss: 0.3780 - acc: 0.9839\n",
      "Epoch 8/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.3129 - acc: 0.9839\n",
      "Epoch 9/30\n",
      "960/960 [==============================] - 0s 238us/step - loss: 0.2560 - acc: 0.9839\n",
      "Epoch 10/30\n",
      "960/960 [==============================] - 0s 234us/step - loss: 0.2097 - acc: 0.9844\n",
      "Epoch 11/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.1741 - acc: 0.9844\n",
      "Epoch 12/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.1479 - acc: 0.9844\n",
      "Epoch 13/30\n",
      "960/960 [==============================] - 0s 242us/step - loss: 0.1287 - acc: 0.9844\n",
      "Epoch 14/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.1147 - acc: 0.9844\n",
      "Epoch 15/30\n",
      "960/960 [==============================] - 0s 251us/step - loss: 0.1044 - acc: 0.9844\n",
      "Epoch 16/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.0968 - acc: 0.9844\n",
      "Epoch 17/30\n",
      "960/960 [==============================] - 0s 247us/step - loss: 0.0911 - acc: 0.9844\n",
      "Epoch 18/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.0869 - acc: 0.9844\n",
      "Epoch 19/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0835 - acc: 0.9844\n",
      "Epoch 20/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0810 - acc: 0.9844\n",
      "Epoch 21/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0788 - acc: 0.9844\n",
      "Epoch 22/30\n",
      "960/960 [==============================] - 0s 248us/step - loss: 0.0772 - acc: 0.9844\n",
      "Epoch 23/30\n",
      "960/960 [==============================] - 0s 238us/step - loss: 0.0758 - acc: 0.9844\n",
      "Epoch 24/30\n",
      "960/960 [==============================] - 0s 244us/step - loss: 0.0747 - acc: 0.9844\n",
      "Epoch 25/30\n",
      "960/960 [==============================] - 0s 243us/step - loss: 0.0739 - acc: 0.9844\n",
      "Epoch 26/30\n",
      "960/960 [==============================] - 0s 249us/step - loss: 0.0730 - acc: 0.9844\n",
      "Epoch 27/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0724 - acc: 0.9844\n",
      "Epoch 28/30\n",
      "960/960 [==============================] - 0s 241us/step - loss: 0.0718 - acc: 0.9844\n",
      "Epoch 29/30\n",
      "960/960 [==============================] - 0s 238us/step - loss: 0.0714 - acc: 0.9844\n",
      "Epoch 30/30\n",
      "960/960 [==============================] - 0s 245us/step - loss: 0.0710 - acc: 0.9844\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       119\n",
      "           1       0.97      1.00      0.98       121\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      " samples avg       0.98      0.98      0.98       240\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************Accuracy****************************\n",
      "The all of acc score:  [98.75, 98.75, 97.91666666666666, 98.33333333333333, 98.33333333333333]\n",
      "The average score: 98.4167% (+/- 0.3118%)\n",
      "---------------------------F1-Score-----------------------------\n",
      "The all of f1_scores score:  [0.987421163891752, 0.9874980465697765, 0.9791053612160681, 0.983328702417338, 0.9833229101521784]\n",
      "The average score: 0.9841% (+/- 0.0031%)\n",
      "+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\n",
      "The all of recall_scores score:  [0.9877622377622378, 0.9878048780487805, 0.9806201550387597, 0.9832627265782345, 0.9831932773109244]\n",
      "The average score: 0.9845% (+/- 0.0028%)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD,Adam\n",
    "#-*-coding:utf-8-*-\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import merge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import f1_score,recall_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 定义 K-fold 交叉验证 参数\n",
    "kfold= KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "n_classes=2\n",
    "flag_nn=1\n",
    "f1_scores=[]\n",
    "recall_scores=[]\n",
    "cvscores=[]\n",
    "for train, test in kfold.split(hybrid_data, y_text_train):\n",
    "    #accuracy_scores = []\n",
    "    \n",
    "    model = Sequential()  \n",
    "    model.add(Dense(64, input_shape=(4,)))\n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid')) \n",
    "    sgd = SGD(lr=0.01)\n",
    "    model.compile(loss='binary_crossentropy',  optimizer=sgd,\n",
    "                  metrics=['accuracy'])  \n",
    "\n",
    "    model.fit(hybrid_data[train], y_text_train[train], batch_size=16, epochs=30,verbose=1)\n",
    "    model.save(\"/home/aqts/yangHong/first-spam-experiment/h5_model/hybrid_for_equal\"+str(flag_nn)+\".h5\")\n",
    "    \n",
    "    # 评估模型\n",
    "    # accuracy\n",
    "    scores = model.evaluate(hybrid_data[test], y_text_train[test], verbose=0)\n",
    "    cvscores.append(scores[1]*100)\n",
    "    y_pred_score=y_pred=model.predict(hybrid_data[test], 16)\n",
    "    #get precision\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i][0]>0.5):\n",
    "            y_pred[i][0]=1\n",
    "        else:\n",
    "            y_pred[i][0]=0\n",
    "        if(y_pred[i][1]>0.5):\n",
    "            y_pred[i][1]=1\n",
    "        else:\n",
    "            y_pred[i][1]=0\n",
    "    print(\"---------------------------Precision-----------------------------\")\n",
    "    print(classification_report(y_text_train[test],y_pred))\n",
    "    #f1-score\n",
    "    f1_scores.append(f1_score(y_text_train[test],y_pred,average = 'macro'))\n",
    "    #recall-score\n",
    "    recall_scores.append(recall_score(y_text_train[test],y_pred,average = 'macro'))\n",
    "    #get auc\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_text_train[test][:, i], y_pred_score[:, i])    \n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])   \n",
    "\n",
    "    # #Plot of a ROC curve for a specific class\n",
    "    plt.rcParams['figure.figsize']=(8,5)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[1], tpr[1], color='darkorange', label='ROC curve spam(auc = %0.4f)' % roc_auc[1])\n",
    "    plt.plot(fpr[0], tpr[0], color='green', label='ROC curve ham (auc = %0.4f)' % roc_auc[0])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC graph for predicting hybrid dataset using the MMA-MF model')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(\"/home/aqts/yangHong/first-spam-experiment/experiment_result_roc/ROC_graph_for_hybrid_dataset_equal\"+str(flag_nn)+\".png\")\n",
    "    plt.show()\n",
    "    flag_nn=flag_nn+1\n",
    "\n",
    "print(\"***************************Accuracy****************************\")\n",
    "print(\"The all of acc score: \" ,(cvscores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"---------------------------F1-Score-----------------------------\")\n",
    "print(\"The all of f1_scores score: \" ,(f1_scores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "print(\"+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\")\n",
    "print(\"The all of recall_scores score: \" ,(recall_scores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing   \n",
    "import numpy as np\n",
    "# y_hybrid_pred = model.predict(hybrid_data_test,batch_size=32, verbose=0)\n",
    "y_text_train_new=[]\n",
    "for i in range(len(y_text_train)):\n",
    "    if(y_text_train[i][0]>0.5):\n",
    "        y_text_train_new.append(1)\n",
    "    else:\n",
    "        y_text_train_new.append(0)\n",
    "y_text_train_new=np.array(y_text_train_new)\n",
    "print(y_text_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优参数:  {'C': 1, 'gamma': 0.001}\n",
      "最佳性能:  0.9841666666666666\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SVM=Support Vector Machine 是支持向量\n",
    "SVC=Support Vector Classification就是支持向量机用于分类，\n",
    "SVC=Support Vector Regression.就是支持向量机用于回归分析\n",
    "'''\n",
    "\n",
    "'''\n",
    "算法（python-sklearn）\n",
    "SVM模型的几种\n",
    "svm.LinearSVC Linear Support Vector Classification.\n",
    "svm.LinearSVR Linear Support Vector Regression.\n",
    "svm.NuSVC Nu-Support Vector Classification.\n",
    "svm.NuSVR Nu Support Vector Regression.\n",
    "svm.OneClassSVM Unsupervised Outlier Detection.\n",
    "svm.SVC C-Support Vector Classification.\n",
    "svm.SVR Epsilon-Support Vector Regression.\n",
    "'''\n",
    "#svm  参数选择 开始调优使用GridSearchCV找到,最优参数\n",
    "\n",
    "# X, y = load_digits(return_X_y = True)\n",
    "parameters = {'gamma': [0,0.001, 0.01], 'C':[1,5,10]}\n",
    "#n_jobs =-1使用全部CPU并行多线程搜索\n",
    "gs = GridSearchCV(SVC(), parameters, refit = True, cv = 5, verbose = 0)\n",
    "gs.fit(hybrid_data, y_text_train_new) #Run fit with all sets of parameters.\n",
    "print('最优参数: ',gs.best_params_)\n",
    "print('最佳性能: ', gs.best_score_)\n",
    "# gs.fit(hybrid_data, y_text_train, batch_size=32, epochs=20,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       119\n",
      "           1       0.98      0.98      0.98       121\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       115\n",
      "           1       0.99      0.98      0.99       125\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       240\n",
      "   macro avg       0.99      0.99      0.99       240\n",
      "weighted avg       0.99      0.99      0.99       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       114\n",
      "           1       0.99      0.96      0.98       126\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       240\n",
      "   macro avg       0.97      0.98      0.97       240\n",
      "weighted avg       0.98      0.97      0.98       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       128\n",
      "           1       1.00      0.98      0.99       112\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       240\n",
      "   macro avg       0.99      0.99      0.99       240\n",
      "weighted avg       0.99      0.99      0.99       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       124\n",
      "           1       0.99      0.97      0.98       116\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "***************************Accuracy****************************\n",
      "The all of acc score:  [97.91666666666666, 98.75, 97.5, 99.16666666666667, 97.91666666666666]\n",
      "The average score: 98.2500% (+/- 0.6124%)\n",
      "---------------------------F1-Score-----------------------------\n",
      "The all of f1_scores score:  [0.9791663049705723, 0.9874823971209512, 0.9749721913236931, 0.9916195265032474, 0.9791228100697646]\n",
      "The average score: 0.9825% (+/- 0.0061%)\n",
      "+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\n",
      "The all of recall_scores score:  [0.9791999444405861, 0.9876521739130435, 0.9757727652464494, 0.9910714285714286, 0.978726362625139]\n",
      "The average score: 0.9825% (+/- 0.0058%)\n"
     ]
    }
   ],
   "source": [
    "# svc get performance\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 定义 K-fold 交叉验证 参数\n",
    "kfold= KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "n_classes=2\n",
    "flag_nn=1\n",
    "f1_scores=[]\n",
    "recall_scores=[]\n",
    "cvscores=[]\n",
    "for train, test in kfold.split(hybrid_data, y_text_train_new):\n",
    "    svc_model=SVC(C=1,gamma= 1)     #SVC(gamma=10,C=1)\n",
    "    svc_model.fit(hybrid_data[train],y_text_train_new[train])\n",
    "    \n",
    "    # 评估模型\n",
    "    # accuracy\n",
    "    scores = svc_model.score(hybrid_data[test], y_text_train_new[test])\n",
    "    cvscores.append(scores*100)\n",
    "    y_pred_score=y_pred=svc_model.predict(hybrid_data[test])\n",
    "    print(\"---------------------------Precision-----------------------------\")\n",
    "    print(classification_report(y_text_train_new[test],y_pred))\n",
    "    #f1-score\n",
    "    f1_scores.append(f1_score(y_text_train_new[test],y_pred,average = 'macro'))\n",
    "    #recall-score\n",
    "    recall_scores.append(recall_score(y_text_train_new[test],y_pred,average = 'macro'))\n",
    "\n",
    "\n",
    "print(\"***************************Accuracy****************************\")\n",
    "print(\"The all of acc score: \" ,(cvscores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"---------------------------F1-Score-----------------------------\")\n",
    "print(\"The all of f1_scores score: \" ,(f1_scores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "print(\"+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\")\n",
    "print(\"The all of recall_scores score: \" ,(recall_scores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score is: 0.9841666666666666\n",
      "best params are: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 3, 'weights': 'uniform'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 320 out of 320 | elapsed:    5.4s finished\n"
     ]
    }
   ],
   "source": [
    "#knn 参数选择 开始调优使用GridSearchCV找到,最优参数\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "#设置k的范围\n",
    "k_range = list(range(1,5))\n",
    "leaf_range = list(range(1,3))\n",
    "weight_options = ['uniform','distance']\n",
    "algorithm_options = ['auto','ball_tree','kd_tree','brute']\n",
    "param_gridknn = dict(n_neighbors = k_range,weights = weight_options,algorithm=algorithm_options,leaf_size=leaf_range)\n",
    "gridKNN = GridSearchCV(KNeighborsClassifier(),param_gridknn,cv=5,scoring='accuracy',verbose=1)\n",
    "gridKNN.fit(hybrid_data, y_text_train_new)\n",
    "print('best score is:',str(gridKNN.best_score_))\n",
    "print('best params are:',str(gridKNN.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       119\n",
      "           1       0.98      0.98      0.98       121\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       115\n",
      "           1       0.97      0.98      0.98       125\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       240\n",
      "   macro avg       0.98      0.97      0.97       240\n",
      "weighted avg       0.98      0.97      0.97       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       114\n",
      "           1       0.98      0.98      0.98       126\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       128\n",
      "           1       0.98      0.98      0.98       112\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       124\n",
      "           1       0.98      0.97      0.97       116\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       240\n",
      "   macro avg       0.98      0.97      0.97       240\n",
      "weighted avg       0.98      0.97      0.97       240\n",
      "\n",
      "***************************Accuracy****************************\n",
      "The all of acc score:  [97.91666666666666, 97.5, 97.91666666666666, 98.33333333333333, 97.5]\n",
      "The average score: 97.8333% (+/- 0.3118%)\n",
      "---------------------------F1-Score-----------------------------\n",
      "The all of f1_scores score:  [0.9791663049705723, 0.9749373433583959, 0.979105361216068, 0.9832589285714286, 0.9749565217391304]\n",
      "The average score: 0.9783% (+/- 0.0031%)\n",
      "+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\n",
      "The all of recall_scores score:  [0.9791999444405861, 0.9746086956521739, 0.97890559732665, 0.9832589285714286, 0.974694104560623]\n",
      "The average score: 0.9781% (+/- 0.0032%)\n"
     ]
    }
   ],
   "source": [
    "# knn get performance\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 定义 K-fold 交叉验证 参数\n",
    "kfold= KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "n_classes=2\n",
    "flag_nn=1\n",
    "f1_scores=[]\n",
    "recall_scores=[]\n",
    "cvscores=[]\n",
    "for train, test in kfold.split(hybrid_data, y_text_train_new):\n",
    "    knn_model=KNeighborsClassifier(algorithm=\"auto\",leaf_size=1,n_neighbors=1,weights=\"uniform\")     #SVC(gamma=10,C=1)\n",
    "    knn_model.fit(hybrid_data[train],y_text_train_new[train])\n",
    "    \n",
    "    # 评估模型\n",
    "    # accuracy\n",
    "    scores = knn_model.score(hybrid_data[test], y_text_train_new[test])\n",
    "    cvscores.append(scores*100)\n",
    "    y_pred_score=y_pred=knn_model.predict(hybrid_data[test])\n",
    "    print(\"---------------------------Precision-----------------------------\")\n",
    "    print(classification_report(y_text_train_new[test],y_pred))\n",
    "    #f1-score\n",
    "    f1_scores.append(f1_score(y_text_train_new[test],y_pred,average = 'macro'))\n",
    "    #recall-score\n",
    "    recall_scores.append(recall_score(y_text_train_new[test],y_pred,average = 'macro'))\n",
    "\n",
    "\n",
    "print(\"***************************Accuracy****************************\")\n",
    "print(\"The all of acc score: \" ,(cvscores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"---------------------------F1-Score-----------------------------\")\n",
    "print(\"The all of f1_scores score: \" ,(f1_scores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "print(\"+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\")\n",
    "print(\"The all of recall_scores score: \" ,(recall_scores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "[CV] max_depth=2, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=25 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... max_depth=2, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=2, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=2, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=2, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=2, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=3, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=3, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=3, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=3, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=3, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=3, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=3, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=3, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=4, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=4, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=4, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=4, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=4, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=5, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=6, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=25 ....................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... max_depth=6, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=6, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=6, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=6, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=6, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=6, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=6, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=6, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=6, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=6, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=6, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=7, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=7, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=7, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=7, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=7, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=7, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=7, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=7, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=7, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=7, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=7, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=7, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=8, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=25 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=25, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=50, total=   0.1s\n",
      "[CV] max_depth=8, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=8, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=8, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=8, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=8, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=8, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=8, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=8, n_estimators=100, total=   0.2s\n",
      "[CV] max_depth=8, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=8, n_estimators=100, total=   0.2s\n",
      "best score is: 0.9816666666666667\n",
      "best params are: {'max_depth': 2, 'n_estimators': 25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 105 out of 105 | elapsed:   15.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV #网格搜索模块\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()#候选参数：树的数量，最大树深，选择的变量树\n",
    "parameters = {'n_estimators':np.array([25,50,100]),'max_depth':np.array([2,3,4,5,6,7,8])}#网格参数搜索，输入之前的模型流程pipe_process,候选参数parameters，并且设置5折交叉验证\n",
    "gs_RF = GridSearchCV(clf,parameters,verbose=2,refit=True,cv=5) #设置备选参数组\n",
    "gs_RF.fit(hybrid_data, y_text_train_new) #模型训练过程\n",
    "# print(gs_RF.best_params_,gs_RF.best_score_) #查看最佳参数和评分（准确度）\n",
    "print('best score is:',str(gs_RF.best_score_))\n",
    "print('best params are:',str(gs_RF.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       119\n",
      "           1       0.98      0.98      0.98       121\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       115\n",
      "           1       0.98      0.99      0.99       125\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       240\n",
      "   macro avg       0.99      0.99      0.99       240\n",
      "weighted avg       0.99      0.99      0.99       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       114\n",
      "           1       0.99      0.96      0.98       126\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       240\n",
      "   macro avg       0.97      0.98      0.97       240\n",
      "weighted avg       0.98      0.97      0.98       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       128\n",
      "           1       1.00      0.99      1.00       112\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       240\n",
      "   macro avg       1.00      1.00      1.00       240\n",
      "weighted avg       1.00      1.00      1.00       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       124\n",
      "           1       0.99      0.97      0.98       116\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "***************************Accuracy****************************\n",
      "The all of acc score:  [97.91666666666666, 98.75, 97.5, 99.58333333333333, 97.91666666666666]\n",
      "The average score: 98.3333% (+/- 0.7454%)\n",
      "---------------------------F1-Score-----------------------------\n",
      "The all of f1_scores score:  [0.9791663049705723, 0.9874736860418587, 0.9749721913236931, 0.9958123222418035, 0.9791228100697646]\n",
      "The average score: 0.9833% (+/- 0.0075%)\n",
      "+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\n",
      "The all of recall_scores score:  [0.9791999444405861, 0.987304347826087, 0.9757727652464494, 0.9955357142857143, 0.978726362625139]\n",
      "The average score: 0.9833% (+/- 0.0072%)\n"
     ]
    }
   ],
   "source": [
    "# random forest get performance\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 定义 K-fold 交叉验证 参数\n",
    "kfold= KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "n_classes=2\n",
    "flag_nn=1\n",
    "f1_scores=[]\n",
    "recall_scores=[]\n",
    "cvscores=[]\n",
    "for train, test in kfold.split(hybrid_data, y_text_train_new):\n",
    "    rfc_model=RandomForestClassifier(max_depth=2,n_estimators=25)     #SVC(gamma=10,C=1)\n",
    "    rfc_model.fit(hybrid_data[train],y_text_train_new[train])\n",
    "    \n",
    "    # 评估模型\n",
    "    # accuracy\n",
    "    scores = rfc_model.score(hybrid_data[test], y_text_train_new[test])\n",
    "    cvscores.append(scores*100)\n",
    "    y_pred_score=y_pred=rfc_model.predict(hybrid_data[test])\n",
    "    print(\"---------------------------Precision-----------------------------\")\n",
    "    print(classification_report(y_text_train_new[test],y_pred))\n",
    "    #f1-score\n",
    "    f1_scores.append(f1_score(y_text_train_new[test],y_pred,average = 'macro'))\n",
    "    #recall-score\n",
    "    recall_scores.append(recall_score(y_text_train_new[test],y_pred,average = 'macro'))\n",
    "\n",
    "print(\"***************************Accuracy****************************\")\n",
    "print(\"The all of acc score: \" ,(cvscores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"---------------------------F1-Score-----------------------------\")\n",
    "print(\"The all of f1_scores score: \" ,(f1_scores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(f1_scores), np.std(f1_scores)))\n",
    "print(\"+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\")\n",
    "print(\"The all of recall_scores score: \" ,(recall_scores))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(recall_scores), np.std(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV #网格搜索模块\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "rnc = NearestCentroid()#候选参数：树的数量，最大树深，选择的变量树\n",
    "#parameters = {'radius':np.array([80,160,320]),\"algorithm\":[\"auto\",\"ball_tree\",\"kd_tree\", \"brute\"]}#网格参数搜索，输入之前的模型流程pipe_process,候选参数parameters，并且设置5折交叉验证\n",
    "#gs_rnc = GridSearchCV(rnc,parameters,verbose=2,refit=True,cv=5) #设置备选参数组\n",
    "rnc.fit(hybrid_data, y_text_train_2) #模型训练过程\n",
    "# print(gs_RF.best_params_,gs_RF.best_score_) #查看最佳参数和评分（准确度）\n",
    "print('best score is:',str(rnc.best_score_))\n",
    "print('best params are:',str(rnc.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       130\n",
      "           1       0.98      0.99      0.99       110\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       240\n",
      "   macro avg       0.99      0.99      0.99       240\n",
      "weighted avg       0.99      0.99      0.99       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       117\n",
      "           1       1.00      0.98      0.99       123\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       240\n",
      "   macro avg       0.99      0.99      0.99       240\n",
      "weighted avg       0.99      0.99      0.99       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       111\n",
      "           1       1.00      0.95      0.97       129\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       240\n",
      "   macro avg       0.97      0.97      0.97       240\n",
      "weighted avg       0.97      0.97      0.97       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       121\n",
      "           1       0.99      0.97      0.98       119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "---------------------------Precision-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       121\n",
      "           1       1.00      0.97      0.98       119\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       240\n",
      "   macro avg       0.98      0.98      0.98       240\n",
      "weighted avg       0.98      0.98      0.98       240\n",
      "\n",
      "***************************Accuracy****************************\n",
      "The all of acc score:  [98.75, 98.75, 97.08333333333333, 98.33333333333333, 98.33333333333333]\n",
      "The average score: 98.2500% (+/- 0.6124%)\n",
      "---------------------------F1-Score-----------------------------\n",
      "The all of f1_scores_new score:  [0.987421163891752, 0.9874980465697765, 0.9707719340976704, 0.983328702417338, 0.9833229101521784]\n",
      "The average score: 0.9825% (+/- 0.0061%)\n",
      "+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\n",
      "The all of recall_scores_new score:  [0.9877622377622378, 0.9878048780487805, 0.9728682170542635, 0.9832627265782345, 0.9831932773109244]\n",
      "The average score: 0.9830% (+/- 0.0055%)\n"
     ]
    }
   ],
   "source": [
    "# random forest get performance\n",
    "from sklearn.model_selection import GridSearchCV #网格搜索模块\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score,recall_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 定义 K-fold 交叉验证 参数\n",
    "kfold= KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "n_classes=2\n",
    "flag_nn=1\n",
    "f1_scores_new=[]\n",
    "recall_scores_new=[]\n",
    "cvscores_new=[]\n",
    "for train, test in kfold.split(hybrid_data, y_text_train_new):\n",
    "    rnc_model = NearestCentroid(metric='manhattan', shrink_threshold=None)  \n",
    "    rnc_model.fit(hybrid_data[train],y_text_train_new[train])\n",
    "    \n",
    "    # 评估模型\n",
    "    # accuracy\n",
    "    scores = rnc_model.score(hybrid_data[test], y_text_train_new[test])\n",
    "    cvscores_new.append(scores*100)\n",
    "    y_pred_score=y_pred=rnc_model.predict(hybrid_data[test])\n",
    "    print(\"---------------------------Precision-----------------------------\")\n",
    "    print(classification_report(y_text_train_new[test],y_pred))\n",
    "    #f1-score\n",
    "    f1_scores_new.append(f1_score(y_text_train_new[test],y_pred,average = 'macro'))\n",
    "    #recall-score\n",
    "    recall_scores_new.append(recall_score(y_text_train_new[test],y_pred,average = 'macro'))\n",
    "\n",
    "print(\"***************************Accuracy****************************\")\n",
    "print(\"The all of acc score: \" ,(cvscores_new))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(cvscores_new), np.std(cvscores_new)))\n",
    "print(\"---------------------------F1-Score-----------------------------\")\n",
    "print(\"The all of f1_scores_new score: \" ,(f1_scores_new))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(f1_scores_new), np.std(f1_scores_new)))\n",
    "print(\"+++++++++++++++++++++++++++Recall-Score+++++++++++++++++++++++++\")\n",
    "print(\"The all of recall_scores_new score: \" ,(recall_scores_new))\n",
    "print(\"The average score: %.4f%% (+/- %.4f%%)\" % (np.mean(recall_scores_new), np.std(recall_scores_new)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
